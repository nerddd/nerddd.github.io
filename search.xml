<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[手册&指南]]></title>
    <url>%2F2018%2F04%2F27%2F%E6%89%8B%E5%86%8C%26%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Linux操作相关##基本操作打包：​ tar -cvf xx.tar xxx​ tar zcvf xx.tar.gz xx解压： tar zxvf xx.tar.gz tar xvf xx.tar tar jxvf xx.tar.bz2 bzip2 -d xx.bz2创建目录xx：mkdir xx创建目录a及以下子目录b:mkdir -p a/b创建xx文件:touch xx查看xx文件:cat xx查看xx文件:less xx(内容多于一屏时，j/k向下/上翻滚)显示绝对路径：pwd删除空目录：rmdir删除xx文件：rm xx强制删除目录a:rm -rf a删除目录a:rm -r a删除父目录a中所有的空子目录：cd a; rmdir复制文件a到文件b：cp a b复制目录a为目录b：cp -r a/ b/从指定文件搜索指定文件：grep &#39;指定内容&#39; ./ -R下载文件到xx目录：axel http://... -o xx给文件a.c加执行权限：chmod +x a.c运行脚本xx.sh:sh xx.sh使.bashrc生效：source ~/.bashrc查看opencv版本：pkg-config --modversion opencv查看opencv安装位置：pkg-config -cflags opencv查找：grep -n -H -R &quot;you want to search &quot; *查看文件夹大小：du -sh查看文件夹大小并排序：du -h /home/* | sort查看文件夹大小：du -h --max-depth=1 ./查看磁盘的使用情况：df -h查看权限：ls -l filaname查看GPU使用率：nvidia-smi查看CPU使用率：top查看文件夹内文件个数：ls -l | grep &#39;^-&#39; | wc -l查看文件夹内目录个数：ls -l | grep &#39;^d&#39; | wc -l查看文件行数：wc -l filename批量更改图片尺寸：mogrify -resize 224x224 -format jpg * 查看网络设置：ifconfig查看操作系统：uname -a查看ubuntu版本：lsb_release -a查看操作系统位数：getconf LONG_BIT查看gcc版本：ls /usr/bin/gcc*或gcc -v查看磁盘使用情况：baobab添加用户：sudo adduser username删除用户: sudo userdel -r username添加sudo权限：sudo vi /etc/sudoers 显示终端上所有进程：ps -a查看进程所有者及其他详细信息：ps -u杀掉某进程：kill -SIGKILL 进程号查看使用apt-get的进程：ps -aux | grep &#39;apt-get&#39;进程暂停：kill -STOP pid进程重启：kill -CONT pid 替换gcc版本：sudo apt-get install gcc-4.9 gcc-4.9-multilib g++-4.9 g++-4.9-multilibsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 40sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5.4 50sudo update-alternatives --config gccsudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 50sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 40sudo update-alternatives --remove gcc /usr/bin/gcc-4.9 查找命令： find . -name ‘my*’：搜索当前目录（含子目录，以下同）中，所有文件名以my开头的文 locate ~/m ：搜索用户主目录下，所有以m开头的文件 locate -i ~/m：搜索用户主目录下，所有以m开头的文件，并且忽略大小写 whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息 which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令 Zip相关： 把一个文件abc.txt和一个目录dir1压缩成为yasuo.zip：zip -r yasuo.zip abc.txt dir1 下载了一个yasuo.zip文件，想解压缩：unzip yasuo.zip 当前目录下有abc1.zip，abc2.zip和abc3.zip，一起解压缩它们：unzip abc\?.zip(注释：?表示一个字符，如果用*表示任意多个字符。) 有一个很大的压缩文件large.zip，不想解压缩，只想看看它里面有什么：unzip -v large.zip 下载了一个压缩文件large.zip，想验证一下这个压缩文件是否下载完全了: unzip -t large.zip 用-v选项发现music.zip压缩文件里面有很多目录和子目录，并且子目录中其实都是歌曲mp3文件，把这些文件都下载到第一级目录，而不是一层一层建目录： unzip -j music.zip Vim相关跳转到指令行：在命令行模式下输入 “:行号”查找字符串：在命令行模式下输入 “/字符串”，按“n”键查找下一个批量替换：在命令行模式下输入 “:%s#abc#def#g” 将def替换abc批量注释：ctrl+v进入列模式，大写I进入插入模式，输入注释符//或#,连按两次esc。取消批量注释：Ctrl + v 进入块选择模式，选中你要删除的行首的注释符号，注意// 要选中两个，选好之后按d即可删除注释 安装驱动查看显卡驱动信息：cat /proc/driver/nvidia/version查询合适的驱动版本xxx: https://www.geforce.com/drivers12345678sudo apt-get remove --purge nvidia-*sudo add-apt-repository ppa:graphics-drivers/ppasudo apt-get updatesudo service lightdm stopsudo apt-get install nvidia-XXXsudo service lightdm startsudo rebootlsmod | grep nvidia #查看驱动状态是否正常 Matlab相关后台运行：nohup matlab -nojvm -nodisplay -nosplash &lt; matlabscript.m 1&gt;running.log 2&gt;running.err &amp; Python相关##路径import os先定义一个带路径的文件filename = “/home/mydir/test.txt”将文件路径分割出来file_dir = os.path.split(filename )[0]判断文件路径是否存在，如果不存在，则创建，此处是创建多级目录12if not os.path.isdir(file_dir): os.makedirs(file_dir) 然后再判断文件是否存在，如果不存在，则创建12if not os.path.exists(filename ): os.system(r&apos;touch %s&apos; % filename) git相关设置Git的user name和email 12git config --global user.name &quot;yourname&quot;git config --global user.email &quot;youremail&quot; 生成SSH密钥 12345678910查看是否已经有了ssh密钥：cd ~/.ssh如果没有密钥则不会有此文件夹，有则备份删除生存密钥：ssh-keygen -t rsa -C “haiyan.xu.vip@gmail.com”按3个回车，密码为空。Your identification has been saved in /home/tekkub/.ssh/id_rsa.Your public key has been saved in /home/tekkub/.ssh/id_rsa.pub.The key fingerprint is:………………最后得到了两个文件：id_rsa和id_rsa.pub 添加密钥到ssh：ssh-add 文件名,需要之前输入密码. 在github上添加ssh密钥，这要添加的是“id_rsa.pub”里面的公钥。 从本地上传到github:git initgit remote add origin https://github.com/nerddd/text.git]]></content>
      <categories>
        <category>技能</category>
      </categories>
      <tags>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人脸识别回顾]]></title>
    <url>%2F2018%2F01%2F29%2F%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E5%9B%9E%E9%A1%BE%2F</url>
    <content type="text"><![CDATA[数据准备数据清洗：去除错误标签，图片质量不好的样本，如果有benchmark，还需要去除和benchmark重复的样本。 构造验证集: 对于给定的数据集，可能没有划分train set和validation set ，需要手动从给定的训练集中按照一定的比例分离出验证集（比如9:1） 数据均衡：数据集可能存在类别不均衡的问题，可以通过重新组合不均衡数据, 使之均衡，方式一: 复制或者合成（比如jitter操作）少数部分的样本, 使之和多数部分差不多数量； 方式二: 砍掉一些多数部分, 使两者数量差不多 数据扩充：对于一些样本数据比较少的数据集，为了更好的训练网络，有时候需要人为增加一些训练样本，比如随机的剪裁、缩放和旋转等。 预处理：常见的就是减均值、除方差。 训练模型####1. 模型选择： 根据具体任务和数据集规模选择合适的网络结构，对于分类任务来说，如果数据集的规模不大，则网络的层数不应太深，结构也不应太复杂。 ####2. 激励函数的选择 sigmoid函数：取值范围为(0,1)，可以将一个实数映射到(0,1)的区间，可以用来做二分类，在特征相差比较复杂或是相差不是特别大时效果比较好，缺点是：激活函数计算量大，反向传播求梯度时，求导涉及除法，反向传播时，很容易出现梯度消失的情况 Tanh函数：取值范围为[-1,1]，在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果，与sigmoid的区别是,tanh是0均值的，因此实际应用中，tanh会比sigmoid更好。 ReLU函数：使用ReLU得到的SGD的收敛速度会比sigmoid/tanh快很多，缺点是训练的时候很”脆弱”，很容易就”die”了例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。 选择的时候，就是根据各个函数的优缺点来配置，例如： 如果使用 ReLU，要小心设置 learning rate，注意不要让网络出现很多 “dead” 神经元，如果不好解决，可以试试 Leaky ReLU、PReLU 或者 Maxout. 详细细节请参看：常用激活函数比较 ​ ####3. 卷积tricks 图片输入是2的幂次方，例如32、64、96、224等。 卷积核大小是33或者55。 输入图片上下左右需要用0补充，即padding，且假如卷积核大小是5，那么padding就是2（图片左右上下都补充2）；卷积核大小是3，padding大小就是1。 ####4. pooling层tricks poolin层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。max pooling比avg pooling效果会好一些。avg-global pooling进入全卷积时代。 5. Loss函数的选择 softmax loss contrastive loss triplet loss center loss triplet loss比softmax的优势 在于softmax不直接，（三元组直接优化距离），因而性能也不好。 softmax产生的特征表示向量都很大，一般超过1000维。 利用triplet loss的metric learning目的在于减小类内的L2距离，同时增大类间的距离 center loss VS triplet loss triplet loss:dramatic data expansion center loss:more directly and efficiently 神经网络的设计模式 Architectural Structure follows the Application（架构遵循应用） 应该根据自己的应用场景选择合适的网络架构。 Proliferate Paths（路径扩增） 每年ImageNet Challenge的赢家都比上一年的冠军使用更加深层的网络。从AlexNet 到Inception到Resnets，Smith和他的团队也观察到“网络的路径数量成倍增长”的趋势，ResNet可以是不同长度的网络的指数集合。 Strive for Simplicity（简洁原则） 更大的并不一定是更好的。在名为“Bigger is not necessarily better”的论文中，Springenberg 等人演示了如何用更少的单元实现最先进的结果。 Increase Symmetry （增加对称性） 无论是在建筑上，还是在生物上，对称性被认为是质量和工艺的标志。Smith 将 FractalNet 的优雅归功于网络的对称性。 Pyramid Shape （金字塔型） 在表征能力和减少冗余或者无用信息之间权衡。CNNs通常会降低激活函数的采样，并会增加从输入层到最终层之间的连接通道。 Over-train （过训练） 另一个权衡是训练准确率和泛化能力。用正则化的方法类似 drop-out 或 drop-path进行提升泛化能力，这是神经网络的重要优势。用比实际用例更难的问题训练你的网络，以提高泛化性能。 Cover the Problem Space （覆盖问题空间） 为了扩大你的训练数据和提升泛化能力，要使用噪声和人工增加训练集的大小，例如随机旋转、裁剪和一些图像操作。 Incremental Feature Construction （递增的功能结构） 当架构变得成功时，它们会简化每一层的“工作”。在非常深的神经网络中，每个 层只会递增地修改输入。在ResNets中，每一层的输出可能类似于输入。所以，在实践中，请在ResNet中使用短的跳过长度。 Normalize Layer Inputs （规范化层输入） 标准化是可以使计算层的工作变得更加容易的一条捷径，并且在实际中可以提升训练的准确性。批量标准化的发明者认为标准化发挥作用的原因在于处理内部的协变量，但Smith 认为，“标准化把所有层的输入样本放在了一个平等的基础上（类似于单位转换），这允许反向传播可以更有效地训练”。 Input Transition （输入转换） 研究表明，在Wide ResNets中,性能随着通道数量的增加而提升，但是要权衡训练成本与准确性。AlexNet，VGG，Inception和ResNets都是在第一层中进行输入变换，以保证多种方式检查输入数据。 Available Resources Guide Layer Widths （可用资源决定层宽度） 可供选择的输出数量并不明显，相应的是，这取决于您的硬件功能和所需的准确性。 Summation Joining （求和连接） Summation是一种流行的合并分支的方式。在 ResNets 中，使用求和作为连接机制可以让每一个分支都能计算残差和整体近似。如果输入跳跃连接始终存在，那么Summation会让每一层学到正确地东西（例如：输入的差别）。在任何分支都可以被丢弃的网络（例如 FractalNet）中，你应该使用这种方式保持输出的平滑。 Down-sampling Transition （下采样变换） 在汇聚的时候，利用级联连接来增加输出的数量。当使用大于1的步幅时，这会同时处理加入并增加通道的数量。 Maxout for Competition （用于竞争的Maxout） Maxout 用在只需要选择一个激活函数的局部竞争网络中。用的方法包含所有的激活函数，不同之处在于 maxout 只选择一个“胜出者”。Maxout 的一个明显的用例是每个分支具有不同大小的内核，而 Maxout 可以尺度不变。 Caffe调参###1. loss为nan(83.3365) [标签错误、学习率太大] 梯度爆炸 原因：梯度变得非常大，使得学习过程难以继续 现象：观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。 措施： 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 设置clip gradient，用于限制过大的diff 不当的损失函数 原因：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。 现象：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。 措施：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。 示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。 不当的输入 原因：输入中就含有NaN。 现象：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。 措施：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。 案例：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。 池化层中步长比核的尺寸大 如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN 1234567891011layer &#123; name: &quot;faulty_pooling&quot; type: &quot;Pooling&quot; bottom: &quot;x&quot; top: &quot;y&quot; pooling_param &#123; pool: AVE stride: 5 kernel: 3 &#125;&#125; ###2. 避免overfitting simpler model structure regularization data augmentation dropall(dropout+drop-path) Bootstrap/Bagging ensemble early stopping utilize invariance Bayesian ​ 人脸识别流程一般流程:数据准备-&gt;人脸检测-&gt;人脸对齐-&gt;生成数据文件-&gt;训练模型-&gt;调参-&gt;model tricks: 在数据准备阶段，可以采用jitter等操作对样本数量较少的种类进行扩充，弥补类别不均衡造成的影响 在生成模型阶段，可以采用ensemble的方式对多个模型进行融合，比如mirror。 参考及致谢 Common causes of nans during training 常用激活函数比较 深度学习之五常见tricks 《Deep convolutional neural network design patterns 》]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行人检测]]></title>
    <url>%2F2017%2F09%2F21%2F%E8%A1%8C%E4%BA%BA%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[文档归类]]></title>
    <url>%2F2017%2F07%2F17%2F%E6%96%87%E6%A1%A3%E5%BD%92%E7%B1%BB%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[DeepID3论文笔记]]></title>
    <url>%2F2017%2F07%2F04%2FDeepID3%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[摘要深层的网络由于良好的学习能力，近年来在目标识别领域取得巨大成功，基于此，提出两个非常深的网络架构，DeepID3，这两个架构分别基于VGG net和GoogLeNet通过改进使之用于人脸识别。Joint face identification-verification supervisory signals are added to both intermediate andfinal feature extraction layers during training. 介绍]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Face</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Center loss笔记]]></title>
    <url>%2F2017%2F06%2F29%2FCenter-loss%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[论文：A Discriminative Feature Learning Approach for Deep Face Recognition 摘要对于一般的CNN网络，softmax通常作为监督信号来训练深层网络，为了增强提取的特征的可辨别性（discriminative），提出center loss，应对人脸识别任务。center loss同时学习每个类别的深层特征中心和惩罚深层特征和它们对应的类中心的距离（the center loss simultaneously learns a center for deepfeatures of each class and penalizes the distances between the deep features and their corresponding class centers）。将softmax和center loss联合起来，可以训练一个健壮的CNN来获取深层特征的两个关键目标，类内紧凑和类间分散。 介绍预先收集所有可能的测试身份用于训练是不现实的，所以CNN的标签预测并不总是适用的。经过深层网络获取得到的特征不仅需要可分开性(separable)，更需要识别性(discriminative)和广义性，足够用来识别新的没有遇见过的类。可识别性的特征可以利用最邻近(NN)或k-NN算法很好的分类，就没有必要依赖标签预测了。但是softmax只能产生可分开(separable)特征，结果特征就不足以用以人脸识别。 因为随机梯度下降(SGD)优化CNN是基于mini-batch，不能很好的反映深层特征的全局分布，由于训练集的庞大，在每次迭代中输入所有的训练样本是不现实的。constractive loss和triplet loss分别作为图像对和三元组的loss函数。然而，与图像样本相比，训练图像对或三元组的数量显著增长，导致收敛缓慢和不稳定性。仔细选择图像对或者三元组，问题可能会部分缓解，但是它增加了计算复杂度，训练过程变得不方便。为了解决这个问题，提出center loss，用于有效的增强特征的可识别性，我们将会得到每个类的深层特征的中心。在训练阶段，我们同时更新中心和最小化特征和它们相应的类中心的距离。CNN同时在softmax loss和center loss的监督下进行训练，通过一个超参来平衡这两个监督信号。直觉上，softmax loss将不同类别的特征分开，center loss有效的将同一类别的特征拉向类的中心，使得类内特征分布变得紧凑。]]></content>
      <categories>
        <category>Face</category>
      </categories>
      <tags>
        <tag>深度学习，论文笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FaceNet论文笔记]]></title>
    <url>%2F2017%2F06%2F28%2FFaceNet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[原文链接:FaceNet:A unified embedding for face recognition and clustering 简介FaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。 前言FaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。 当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。选取的triplets包含两个匹配脸部缩略图（为紧密裁剪的脸部区域，不用使用2d、3d对齐以及放大转换等预处理）和一个非匹配的脸部缩略图，loss函数目标是通过距离边界区分正负类。 本文中，探索了两类深度卷积神经网络，第一类为Zeiler&amp;Fergus研究中使用的神经网络，包含多个交错的卷积层、非线性激励函数，局部相应归一化和最大池化层。我们额外的添加了一些1x1xd的卷积层。第二种结构是基于Inception model，这种网络利用了一些不同的卷积层和池化层并行和级联响应。我们发现这些模型可以减小20倍以上的参数数量，并且可能会减少FLOPS数量。 triplet loss的启发是传统loss函数趋向于将有一类特征的人脸图像映射到同一个空间，而triplet loss尝试将一个个体的人脸图像和其他人脸图像分开。 总结 三元组的目标函数并不是这篇论文首创，我在之前的一些Hash索引的论文中也见过相似的应用。可见，并不是所有的学习特征的模型都必须用softmax。用其他的效果也会好。 三元组比softmax的优势在于 softmax不直接，（三元组直接优化距离），因而性能也不好。 softmax产生的特征表示向量都很大，一般超过1000维。 FaceNet并没有像DeepFace和DeepID那样需要对齐。 FaceNet得到最终表示后不用像DeepID那样需要再训练模型进行分类，直接计算距离就好了，简单而有效。 论文并未探讨二元对的有效性，直接使用的三元对。 参考文献谷歌人脸识别系统FaceNet解析 FaceNet–Google的人脸识别]]></content>
      <categories>
        <category>Face</category>
      </categories>
      <tags>
        <tag>笔记，人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe调参]]></title>
    <url>%2F2017%2F06%2F17%2FCaffe%E8%B0%83%E5%8F%82%2F</url>
    <content type="text"><![CDATA[loss为nan梯度爆炸 原因：梯度变得非常大，使得学习过程难以继续 现象：观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。 措施： 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 设置clip gradient，用于限制过大的diff ## 不当的损失函数 原因：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。 现象：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。 措施：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。 示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。 ## 不当的输入 原因：输入中就含有NaN。 现象：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。 措施：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。 案例：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。 池化层中步长比核的尺寸大 如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN 1234567891011layer &#123; name: &quot;faulty_pooling&quot; type: &quot;Pooling&quot; bottom: &quot;x&quot; top: &quot;y&quot; pooling_param &#123; pool: AVE stride: 5 kernel: 3 &#125;&#125; 致谢 http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training Accuracy一直为0考虑标签是否从0开始递增]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python]]></title>
    <url>%2F2017%2F06%2F09%2FPython%2F</url>
    <content type="text"><![CDATA[图像操作关于PIL image和skimage的图像处理对skimage图像镜像处理： 123456789101112from skimage import io,transformimport matplotlib.pyplot as pltimport cv2img=io.imread(&quot;test.jpg&quot;)#img=transform.rotate(img,180)img=cv2.flip(img,1)plt.figure(&apos;skimage&apos;)plt.imshow(img)plt.show()print img.shapeprint(img.dtype) 文本处理提取两个文件中相同的部分a.txt内容： 123aaa 0bbb 0ccc 0 b.txt内容： 123aaa 0bbb 1ccc 0 提取相同的部分写入到c.txt 1234567891011fa=open(&apos;a.txt&apos;,&apos;r&apos;)a=fa.readlines()fa.close()fb=open(&apos;b.txt&apos;,&apos;r&apos;)b=fb.readlines()fb.close()c= [i for i in a if i in b]fc=open(&apos;c.txt&apos;,&apos;w&apos;)fc.writelines(c)fc.close()print &apos;Done&apos; 最后c.txt内容 12aaa 0ccc 0 读取文件并绘制图片散点图12345678910111213141516171819202122232425262728import matplotlibimport matplotlib.pyplot as pltdef loadData(fileName): inFile=open(fileName,&apos;r&apos;) X = [] y = [] for line in inFile: trainingSet = line.split() X.append(trainingSet[0]) y.append(trainingSet[1]) return (X, y)def plotData(X, y): length = len(y) plt.figure(1) #plt.plot(X, y, &apos;rx&apos;) plt.scatter(X,y,c=&apos;r&apos;,marker=&apos;.&apos;) plt.xlabel(&apos;eye_width&apos;) plt.ylabel(&apos;eye_height&apos;) #plt.show() plt.savefig(&apos;dis.png&apos;)if __name__ == &apos;__main__&apos;: (X, y) = loadData(&apos;dis.txt&apos;) plotData(X, y) 折线图1234567891011121314151617import matplotlib.pyplot as plt y1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] x1=range(0,200,10) num = 0for i in range(20): num+=y1[i]print numplt.plot(x1,y1,label=&apos;Frist line&apos;,linewidth=1,color=&apos;r&apos;,marker=&apos;o&apos;, markerfacecolor=&apos;blue&apos;,markersize=6) plt.xlabel(&apos;eye_width distribute&apos;) plt.ylabel(&apos;Num&apos;) plt.title(&apos;Eye\nCheck it out&apos;) plt.legend()plt.savefig(&apos;figure.png&apos;) plt.show() 统计文件中的数据分布12345678910111213141516import matplotlibimport matplotlib.pyplot as pltimport numpy as npdef loadData(fileName): inFile=open(fileName,&apos;r&apos;) x_lines=inFile.readlines()#x_lines为str的list x_distribute=[0]*20 #对列表元素进行重复复制 for x_line in x_lines: x_point=x_line.split()[0] i=np.int(np.float32(x_point)/10) #注意str要先转化为np.float才能转化为int型 x_distribute[i]+=1 print x_distributeif __name__ == &apos;__main__&apos;: loadData(&apos;dis.txt&apos;) windows下安装OpenCV for Python Download Python, Numpy, OpenCV from their official sites. Extract OpenCV (will be extracted to a folder opencv) Copy ..\opencv\build\python\x86\2.7\cv2.pyd Paste it in C:\Python27\Lib\site-packages Open Python IDLE or terminal, and type 1&gt;&gt;&gt; import cv2 读取文件，进行批量创建目录存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下： 123xxx/0.jpgyyy/0.jpgzzz/0.jpg 创建xxx目录，yyy目录，zzz目录 12345678910111213141516171819202122232425262728293031323334353637import sysimport osimport numpy as npimport skimagefrom skimage import io,transformimport matplotlib.pyplot as pltimport cv2def read_file(path): with open(path) as f: return list(f)def make_dir(image_path): image_lines = read_file(image_path) if not image_lines: print &apos;empty file&apos; return i = 0 for image_line in image_lines: image_line = image_line.strip(&apos;\n&apos;) subdir_name = image_line.split(&apos;/&apos;)[0] print subdir_name isExists=os.path.exists(subdir_name) if not isExists: os.mkdir(subdir_name) print subdir_name+&quot;created successfully!&quot; i = i+1 sys.stdout.write(&apos;\riter %d\n&apos; %(i)) sys.stdout.flush() print &apos;Done&apos;if __name__==&apos;__main__&apos;: image_path=&apos;./image.txt&apos; make_dir(image_path)]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Triplet loss]]></title>
    <url>%2F2017%2F06%2F02%2FTriplet%20loss%2F</url>
    <content type="text"><![CDATA[原理Triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。 Triplet loss中的margin取值分析我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。 12345当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。 相关区分相似图形，除了triplet loss，还有一篇CVPR：《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》提出的Coupled Cluster Loss. 本文参考： triplet loss 原理以及梯度推导 如何在Caffe中增加layer以及Caffe中triplet loss layer的实现]]></content>
      <categories>
        <category>Face</category>
      </categories>
      <tags>
        <tag>深度学习，人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo相关]]></title>
    <url>%2F2017%2F06%2F02%2FHexo%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[问题Hexo无法正常显示公式善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为： 1234# MathJax Supportmathjax: enable: true per_page: true 另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下： 123456---title: index.htmldate: 2016-12-28 21:01:30tags:mathjax: true-- 题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下： 123456title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories: tags:description: mathjax: 优化Hexo的版本控制与持续集成]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[待办及进度]]></title>
    <url>%2F2017%2F06%2F02%2F%E5%BE%85%E5%8A%9E%E5%8F%8A%E8%BF%9B%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[step1:Mirror face相关 Model PCA Size Threshold Score Mirror 192 0.64 99.42% Mirror Concat 192 0.65 99.42% Mirror Add/Average 184 0.64 99.47% Mirror Max 144 0.65 99.43% Mirror Min 168 0.65 99.48% Mirror Avg+min 168 0.65 99.45% step2:单眼Patch model eye_width num eye_width num 0~10 14 90~100 8,5674 10~20 3329 100~110 2,9481 20~30 21,3675 110~120 9051 30~40 45,1416 120~130 2894 40~50 49,1913 130~140 932 50~60 72,8911 140~150 279 60~70 137,9232 150~160 86 70~80 128,7442 160~170 14 80~90 30,9026 170~180 6 问题1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y…],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句12if eye_width*0.5&gt;eye_left_x: eye_width=eye_left_x*2 导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张 重新生成眼睛宽度估算文件，其分布如下 eye_width num eye_width num 0~10 1,8975 100~110 98 10~20 71,3760 110~120 17 20~30 126,4102 120~130 3 30~40 226,4348 130~140 0 40~50 59,3351 140~150 0 50~60 10,6502 150~160 0 60~70 2,4588 160~170 1 70~80 5592 170~180 0 80~90 1588 180~190 8 90~100 421 190~200 21 2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526 17个小时，迭代12万次，26.5%的准确率，loss=5.5。 step3:双眼patch model生成双眼宽度估算文件，其分布如下 eye_width num eye_width num eye_width num 0~10 30 100~110 18,8771 200~210 383 10~20 5888 110~120 8,8508 210~220 196 20~30 11,4595 120~130 4,6243 220~230 100 30~40 35,5451 130~140 2,4982 230~240 52 40~50 42,6729 140~150 1,2749 240~250 33 50~60 49,1312 150~160 6714 250~260 15 60~70 69,0394 160~170 3413 260~270 3 70~80 104,9353 170~180 1909 270~280 2 80~90 100,6618 180~190 1091 280~290 0 90~100 47,7219 190~200 622 290~300 0 筛选crop后，宽度在20~130区间的图片，共90523个类别，4932655张。 train.txt：3982004 val.txt：950661 18万次迭代之后，准确率只有66%左右。 step4:crop对齐后的图片的眼睛，训练单眼模型数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”） &quot;ref_points&quot;: [ 30.2946, 51.6963, 65.5318, 51.5014, 48.0252, 71.7366, 33.5493, 92.3655, 62.7299, 92.2041 ] eye_width=(ref_points[2]-ref_points[0])*0.8 eye_height=eye_width x1=ref_points[0]-0.5*eye_width=16 x2=ref_points[0]+0.5*eye_width=44 y1=ref_points[1]-0.5*eye_height=37 y2=ref_points[1]+0.5*eye_height=65 train:4071324 张 val:973183张 step5:crop对齐后的图片的眼睛，训练双眼模型数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”） &quot;ref_points&quot;: [ 30.2946, 51.6963, 65.5318, 51.5014, 48.0252, 71.7366, 33.5493, 92.3655, 62.7299, 92.2041 ] eye_width=(ref_points[2]-ref_points[0])*0.8 eye_height=eye_width x1=ref_points[0]-0.5*eye_width=16 x2=ref_points[2]+0.5*eye_width=79 y1=ref_points[1]-0.5*eye_height=37 y2=ref_points[1]+0.5*eye_height=65 迭代16万次，精度为72.73%，loss=2.52 在lfw上测试，精度最高达到77.04% step6:Center face+dropout+finetune on softmax在msclean测试集上达到93.53% Model PCA_Size Threshold Score dropcenter 168 0.64 99.42% dropcenter_mirror 136 0.64 99.38% dropcenter +dropcenter_mirror+Min 128 0.64 99.45% dropcenter+dropcenter_mirror+ Add 128 0.64 99.45% center +dropcenter +Min 400 0.64 99.40% centermirror+dropcenter+Min 128 0.64 99.45% centermirror+dropcenter+Add 160 0.64 99.43% centermirror+dropcenter+Max 160 0.64 99.43% centermirror+dropcenter+Concate 192 0.65 99.47% Model PCA Size Threshold Score center_min_mirror+dropcenter+Concate 192 0.65 99.47% center_min_mirror+dropcenter+Min 128 0.65 99.42% center_min_mirror+dropcenter+Add 136 0.64 99.47% eye_model 160 0.57 77.04% eyemodel+center+Con 208 0.66 74.80% 三模型 center+center_min_mirror+dropoutcenter+Concate 128 0.65 99.43% center+softmax+dropoutcenter+Concate 168 0.66 99.43% center+softmax+dropoutcenter+Add 496 0.65 99.42% step7:balancestep7.1:减小过采样的数量，防止过拟合对/home/yf/data/clean.txt中每种类别进行统计各有多少个数： 每种类别包含图片张数 类别数 每种类别包含图片张数 类别数 &lt;10 1213 10~20 11617 20~30 10868 30~40 9692 40~50 9020 50~60 8426 60~70 8443 70~80 8783 80~90 8762 90~100 7317 100~110 4277 110~120 1753 120~130 354 类别总数共90525。由上图可知，类别严重不均衡，之前处理类别不均衡的方法主要是欠抽样和过抽样结合，对于多数类样本丢弃一部分样本，对于少数类样本复制生成，最后的训练数据分布如下： 每种类别包含图片张数 类别数 每种类别包含图片张数 类别数 70~80 3673 80~90 19068 90~100 27069 100~110 40715 由于少数类占了大多数，但是重复太多，可能导致过拟合问题，于是将每个类别的图片张数减去30，重新生成balance的训练数据，并训练模型。 迭代17万次后，msdata测试集上准确率达到92.28%，loss=0.27 lfw上精度为99.18% mirror:99.27% add:99.27% step7.2:EasyEmsemble法均衡类别step7.1的方法属于欠抽样和过抽样结合： 对于欠抽样算法，将多数类样本删除有可能会导致分类器丢失有关多数类的重要信息。 对于过抽样算法，虽然只是简单地将复制后的数据添加到原始数据集中，且某些样本的多个实例都是“并列的”，但这样也可能会导致分类器学习出现过拟合现象，对于同一个样本的多个复本产生多个规则条例，这就使得规则过于具体化；虽然在这种情况下，分类器的训练精度会很高，但在位置样本的分类性能就会非常不理想。 EasyEnsemble 核心思想是： 首先通过从多数类中独立随机抽取出若干子集 将每个子集与少数类数据联合起来训练生成多个基分类器 最终将这些基分类器组合形成一个集成学习系统 设立一个阈值50，对于类别样本数超过50的，将其分写到两个不同的文件；对于类别样本数不超过50的，利用过采样进行增添，所以最终得到两个有交集的训练集A,B，两个训练集的样本数都是 90525*50=4526250 训练两个model，然后提取特征，对特征进行融合。 Model(acc/loss) Pca Size Threshold Score model1(90.75%/0.41) 176 0.64 99.05% model1(92.14%/0.26) 200 0.62 99.28% model1(92.14%/0.26) Mirror 280 0.63 99.32% model1(92.14%/0.26) Add Mirror 128 0.65 99.30% model2(92.56%/0.41) 128 0.64 99.27% model2(92.56%/0.41) Mirror 192 0.63 99.35% model2(92.56%/0.41) Add Mirror 192 0.64 99.32% model1 add model2 152 0.64 99.33% model1 mirror add model2 mirror 136 0.65 99.37% model1 add model2 mirror 152 0.64 99.37% model1_add_mirror add model2_add_mirror 152 0.64 99.38% model1_add_mirror concate model2_add_mirror 128 0.65 99.32% model1 mirror min model2 mirror 168 0.64 99.37% step8:UMDFaces对UMDFaces数据集进行人脸对齐处理 batch1:175,534(3554类) batch2:115,126(2590类) batch3:77,228(2133类) frames:3,735,475(3106类) 提取4个数据集的类别名称，经过处理分析后发现frames的类别属于batch1类别的子集，将3个batch与frames的数据集整合到一个数据集下，因为当静态图片和视频帧进行结合后训练的模型往往既能兼顾个体之间的差异（静态图片特征）也能学习到同一个个体的姿态变化（视频帧特征），要注意的一点就是对于frames和batch1中同一个类别的要放在一个目录下，并重新生成类别标签。 数据总量:4103363(8276个类别) 数据整理已经完成，接下来是在这个数据集上进行metric learning的训练。 train:3286012val:817351 step9:Megaface测试 Model Dataset Score(Megaface/LFW) center-face FaceScrub Set1/LFW 67.32%/99.42% balance-reduced FaceScrub Set1/LFW 70.99%/99.18% easyensemble FaceScrub Set1 73.91%/99.33% easyensemble concat addmirror FaceScrub Set1 74.21%/99.37% balance-cent-soft FaceScrub Set1/LFW 74.47%/99.33% balance-cent-soft concat mirror FaceScrub Set1 75.65% step9.1:Megaface测试（续） Model Dataset Score(Megaface/LFW) Dropout_center Concat mirror FaceScrub Set1/LFW 69.34%/99.47% normface easyensemble model1 Concart mirror FaceScrub Set1/LFW 70.32% normface easyensemble 2models Concat mirror FaceScrub Set1 70.49% balance concat mirror FaceScrub(matlab_mtcnn) 78.84% easyensemble concat addmirror FaceScrub(matlab_mtcnn) 75.92% jitter_center_iter_190000 concat mirror FaceScrub(matlab_mtcnn) 77.69% jitter_softmax_iter_180000 concat mirror FaceScrub(matlab_mtcnn) 83.13% jitter_softmax_iter_180000 only mirror FaceScrub(matlab_mtcnn) 82.08% jitter_softmax_iter_180000 add mirror FaceScrub(matlab_mtcnn) 82.87% jitter_softmax_iter_184000 concat mirror FaceScrub(matlab_mtcnn) 83.16% jitter_softmax_iter_180000 concat mirror FaceScrub(python_mtcnn) 78.33% jitter_softmax_iter_184000 concat mirror(matlab align) FaceScrub(matlab_mtcnn) 79.05% normface_jitter_iter_124000 concat mirror(python align) FaceScrub(python_mtcnn) 76.50% normface_jitter_iter124000 cancat mirror(python align) FaceScrub(matlab_mtcnn) 78.92% step 10:MTCNN(matlab)人脸检测及对齐step 10.1：对齐Megaface和FaceScrub主要是Megaface数据集（1028062张）,FaceScrub数据集(91712张)，其中FaceScrub数据集中通过mtcnn（/home/yf/align/align_megaface.m）检测到的有89751张，剩余的1961张需要利用数据集中提供的3个关键点进行对齐，首先需要获取未检测到的图片的路径，然后利用python 脚本(/home/yf/megaface/devkit/templatelists/analysis/analysis_json.py)解析对应的存储该图片中人脸关键点的json文件，最后在利用matlab脚本(/home/yf/align/for_not_detect/align_megaface.m)进行批量对齐。Megaface数据集中未检测到的数据集同样处理。 问题： 在进行了41万次对齐后，出现了imread的错误，然后将从目录读取路径改成了从存储图片路径的文件中(/home/yf/megaface/tests/MegaFace_align_list_image.txt)直接获取路径，并输出每次进行处理的文件名，重新进行对齐操作，然后重现了这个错误，最后比对MegaFace_align_list_image.txt的下一张图片，发现有张图片是输入为空的。 step 10.2:对齐msceleb数据重新对齐msceleb数据集用于训练。 step 11:Normface训练Normface(paper:NormFace: L2 Hypersphere Embedding for Face Verification) step 11.1:训练EasyEnsemble模型model1在测试集上的准确率为92.88%，model2在测试集上的准确率为92.85%。 暂时只测了单个的model1 concate mirror在Megaface(还是原始python版mtcnn对齐的)上的准确率只有70.32%。下周继续测试两个模型的效果。 step 11.2:训练Balance模型刚生成完训练的数据集，下周开始训练。 step 12:Image Jitter对图片增加随机扰动，包括缩放、角度变换、镜像操作，主要还是msceleb数据集(/home/yf/data/msclean)上进行，由于该数据集类别不均衡，所以对于样本数较少的类别可以采用这种办法增加样本容量。最终将每个类别的样本数控制在80~160之间。 10240892 train:9257534 val:983342 正在生成训练的数据集lmdb。 jitter_center_iter_190000 concat mirror FaceScrub(matlab_mtcnn) 77.69% jitter_softmax_iter_180000 concat mirror FaceScrub(matlab_mtcnn) 83.18% jitter_softmax_iter_180000 only mirror FaceScrub(matlab_mtcnn) 82.08% jitter_softmax_iter_180000 add mirror FaceScrub(matlab_mtcnn) 82.87% jitter_softmax_iter_184000 concat mirror FaceScrub(matlab_mtcnn) 83.16% step 13:Gender test1.0VGG16在lfw上准确率90.04%，在imdb(15590测试样本)上准确率90.92% model training:female(69847),male(86061) train:124728 val:15590 test:15590 2.0网络：AlexNet 在清理后的数据集上，迭代29500次后，训练准确率为98.16%，loss=0.55 在测试集上达到98.11%（15127/15418) Todo1 [x] Create umdfaces–&gt;lmdb [x] EasyEnsemble train and test [x] Use matcaffe for metric learning [ ] Megaface test- - [x] center face - [x] balance-cent-soft - [x] reduced - [x] mirror or concatenate - [x] EasyEnsemble [x] Paper reading:One-shot face recognition by promoting underrepresented classes ​ Todo2 [ ] jitter model training(softmax first) [ ] balance model retrain on normface(include center loss) [ ] aligned by matlab_mtcnn megaface(balance model 75.65% version) [ ] gender classfication model training - check the dataset (detect and crop by matlab_mtcnn) - generate lmdb - choose a model(ResNet?) ​ 参考链接happynear-face-verification dlib-jitter dlib-face-verification-blog]]></content>
  </entry>
  <entry>
    <title><![CDATA[杂知识点]]></title>
    <url>%2F2017%2F06%2F01%2F%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[分类与回归本节部分转载于穆文发表于知乎的分类与回归区别是什么下面的回答，获得原作者授权 分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。 Logistic Regression&amp;Linear Regression: Linear Regression:输出一个标量wx+b，这个值是连续值，用以回归问题 Logistic Regression:将上面的wx+b通过sigmoid函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题 对于N分类问题，可以先计算N组w值不同的wx+b ，然后归一化，比如softmax函数变成N个类上的概率，用以多分类 SVR &amp;SVM SVR:输出wx+b，即某个样本点到分类面的距离，是连续值，属于回归问题 SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类 Naive Bayes用来分类和回归 前馈神经网络（CNN系列）用于分类和回归 回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出看做向量v，现全部连接到一个神经元上，这个神经元的输出wv+b，是一个连续值，处理回归问题，和Linear Regression的思想一样 分类：将m个神经元最后连接到N个神经元，有N组不同的wv+b，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题 循环神经网络（RNN系列）用于分类和回归 回归和分类与CNN类似，输出层的值y=wx+b，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列 Logistic回归&amp;SVM 两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。^1 线性模型的表达式为 $$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$$ ​ 将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。 ​ 用n表示Feature数量,m表示训练集个数。下面分情况讨论^2： n很大，m很小n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择线性kernel的SVM，也可以选择Logistic回归。 n很小，m中等n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择非线性kernel的SVM，比如高斯核kernel的SVM。 n很小，m很大n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，带核函数的SVM计算也非常慢。所以此时应该选线性kernel的SVM，也可以选择Logistic回归。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。 一些概念迁移学习^3：有监督预训练(supervised pre-training)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。 NMS(非极大值抑制)： 在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列算法中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。 定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。就这样一直重复，找到所有被保留下来的矩形框。 IoU(交并比)： 物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式$$IoU=\frac{A\cap B}{A\cup B}$$ 准确率&amp;精确率&amp;召回率:​ 准确率是正确预测的样本占总的预测样本比例​ 精确率是预测为正的样本中有多少是真的正类​ 召回率是样本中有多少正例被正确的预测​ F值=准确率*召回率*2/(准确率+召回率)，是准确率和召回率的调和平均值 ​TP：正类被预测为正类​FN：正类被预测为负类​FP：负类被预测为正类​TN：负类被预测为负类 $$准确率=\frac{TP+TN}{TP+TF+FN+FP}$$ $$精确率=\frac{TP}{TP+FP}$$ $$召回率=\frac{TP}{TP+FN}$$ 卷积计算后的图片尺寸：$$outputsize=\frac{imagesize+2*padding-kernelsize}{stride}+1$$ RankBoost:​ RankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类 器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1&gt;r2&gt;r3&gt;r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题 真阳率，假阳率，AUC，ROC $真阳率=\frac{a}{a+c}$:含义是检测出来的真阳性样本数除以所有真实阳性样本数。 $假阳率=\frac{b}{b+d}$:含义是检测出来的假阳性样本数除以所有真实阴性样本数 ROC曲线就是把假阳率当x轴，真阳率当y轴画一个二维平面直角坐标系。然后不断调整检测方法（或机器学习中的分类器）的阈值，即最终得分高于某个值就是阳性，反之就是阴性，得到不同的真阳率和假阳率数值，然后描点。就可以得到一条ROC曲线。需要注意的是，ROC曲线必定起于（0，0），止于（1，1）。因为，当全都判断为阴性(-)时，就是（0，0）；全部判断为阳性(+)时就是（1，1）。这两点间斜率为1的线段表示随机分类器（对真实的正负样本没有区分能力）。所以一般分类器需要在这条线上方 AUC就是ROC曲线下方的面积，越接近1表示分类器越好。 参考文献SVM和logistic回归分别在什么情况下使用 SVM和Logistic的区别 [物体检测中常用的几个概念迁移学习、IOU、NMS理解] 机器学习算法常用指标总结]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>