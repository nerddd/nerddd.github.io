<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Triplet loss]]></title>
    <url>%2F2017%2F06%2F02%2FTriplet%20loss%2F</url>
    <content type="text"><![CDATA[原理Triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。 Triplet loss中的margin取值分析我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。 12345当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。 相关区分相似图形，除了triplet loss，还有一篇CVPR：《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》提出的Coupled Cluster Loss. 本文参考： triplet loss 原理以及梯度推导 如何在Caffe中增加layer以及Caffe中triplet loss layer的实现]]></content>
      <categories>
        <category>Face</category>
      </categories>
      <tags>
        <tag>深度学习，人脸识别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo相关]]></title>
    <url>%2F2017%2F06%2F02%2FHexo%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[问题Hexo无法正常显示公式善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为： 1234# MathJax Supportmathjax: enable: true per_page: true 另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下： 123456---title: index.htmldate: 2016-12-28 21:01:30tags:mathjax: true-- 题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下： 123456title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;categories: tags:description: mathjax: 优化]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[待办及进度]]></title>
    <url>%2F2017%2F06%2F02%2F%E5%BE%85%E5%8A%9E%E5%8F%8A%E8%BF%9B%E5%BA%A6%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[杂知识点]]></title>
    <url>%2F2017%2F06%2F01%2F%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[分类与回归本节部分转载于穆文发表于知乎的分类与回归区别是什么下面的回答，获得原作者授权 分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。 Logistic Regression&amp;Linear Regression: Linear Regression:输出一个标量wx+b，这个值是连续值，用以回归问题 Logistic Regression:将上面的wx+b通过sigmoid函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题 对于N分类问题，可以先计算N组w值不同的wx+b ，然后归一化，比如softmax函数变成N个类上的概率，用以多分类 SVR &amp;SVM SVR:输出wx+b，即某个样本点到分类面的距离，是连续值，属于回归问题 SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类 Naive Bayes用来分类和回归 前馈神经网络（CNN系列）用于分类和回归 回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的三个月抽根烟截图看做向量v，现全部连接到一个神经元上，这个神经元的输出wv+b，是一个连续值，处理回归问题，和Linear Regression的思想一样 分类：将m个神经元最后连接到N个神经元，有N组不同的wv+b，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题 循环神经网络（RNN系列）用于分类和回归 回归和分类与CNN类似，输出层的值y=wx+b，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列 Logistic回归&amp;SVM 两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。^1 线性模型的表达式为 $$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$$ ​ 将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。 ​ 用n表示Feature数量,m表示训练集个数。下面分情况讨论^2： n很大，m很小n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择线性kernel的SVM，也可以选择Logistic回归。 n很小，m中等n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择非线性kernel的SVM，比如高斯核kernel的SVM。 n很小，m很大n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，带核函数的SVM计算也非常慢。所以此时应该选线性kernel的SVM，也可以选择Logistic回归。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。 一些概念迁移学习^3：有监督预训练(supervised pre-training)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。 NMS(非极大值抑制)： 在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列算法中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。 定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。就这样一直重复，找到所有被保留下来的矩形框。 IoU(交并比)： 物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式$$IoU=\frac{A\cap B}{A\cup B}$$ 准确率&amp;精确率&amp;召回率:​ 准确率是正确预测的样本占总的预测样本比例​ 精确率是预测为正的样本中有多少是真的正类​ 召回率是样本中有多少正例被正确的预测​ F值=准确率*召回率*2/(准确率+召回率)，是准确率和召回率的调和平均值 ​TP：正类被预测为正类​FN：正类被预测为负类​FP：负类被预测为正类​TN：负类被预测为负类 $$准确率=\frac{TP+TN}{TP+TF+FN+FP}$$ $$精确率=\frac{TP}{TP+FP}$$ $$召回率=\frac{TP}{TP+FN}$$ 卷积计算后的图片尺寸：$$outputsize=\frac{imagesize+2*padding-kernelsize}{stride}$$ RankBoost:​ RankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类 器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1&gt;r2&gt;r3&gt;r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题 参考文献SVM和logistic回归分别在什么情况下使用 SVM和Logistic的区别 [物体检测中常用的几个概念迁移学习、IOU、NMS理解]]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>