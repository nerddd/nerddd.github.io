<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Calyp</title>
  <subtitle>Study</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="calyp.github.io/"/>
  <updated>2019-03-01T07:09:18.546Z</updated>
  <id>calyp.github.io/</id>
  
  <author>
    <name>Calyp</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>相关数学知识</title>
    <link href="calyp.github.io/2019/02/28/%E7%9B%B8%E5%85%B3%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"/>
    <id>calyp.github.io/2019/02/28/相关数学知识/</id>
    <published>2019-02-28T00:53:22.000Z</published>
    <updated>2019-03-01T07:09:18.546Z</updated>
    
    <content type="html"><![CDATA[<h2 id="统计学相关"><a href="#统计学相关" class="headerlink" title="统计学相关"></a>统计学相关</h2><h3 id="协方差、协方差矩阵"><a href="#协方差、协方差矩阵" class="headerlink" title="协方差、协方差矩阵"></a>协方差、协方差矩阵</h3><p>标准差和方差一般是用来描述一维数据的，针对多维数据，协方差就是用来<strong>度量两个随机变量关系</strong>的统计量，仿照方差的定义</p>
<p><img src="/images/20150930142616971.jpg" alt="015093014261697"></p>
<p>给出协方差的定义</p>
<p><img src="/images/20150930142639675.jpg" alt="015093014263967"></p>
<p>协方差的结果为正值，则表明两者是正相关的，结果为负，则说明两个变量之间的负相关，如果为0，则说明两者之间没有关系，也就是“相互独立”。</p>
<p>协方差只能处理<strong>二维问题</strong>，，随着维数的增加，需要计算多个协方差，所以需要用矩阵组织这些数据，写明是协方差矩阵的定义：</p>
<p><img src="/images/20150930142849768.jpg" alt="015093014284976"></p>
<p>假设数据集有三个维度，则协方差矩阵为：</p>
<p><img src="/images/20150930142912268.jpg" alt="015093014291226"></p>
<p>由上可知，协方差矩阵是一个对称矩阵，对角线为各个维度的方差。</p>
<h3 id="相关系数、相关系数矩阵"><a href="#相关系数、相关系数矩阵" class="headerlink" title="相关系数、相关系数矩阵"></a>相关系数、相关系数矩阵</h3><p>相关系数是用来反映变量之间<strong>相关关系密切程度</strong>的统计指标，相关系数是按积差方法计算，同样以两变量与各自平均值的离差为基础，通过两个离差相乘来反映两变量之间相关程度，计算公式如下</p>
<p><img src="/images/44335b50ca3c7833ed427137d03f0285.png" alt="4335b50ca3c7833ed427137d03f028"></p>
<p>相关系数的值介于–1与+1之间，即–1≤r≤+1。其性质如下：</p>
<ul>
<li>当r&gt;0时，表示两变量正相关，r&lt;0时，两变量为负相关。</li>
<li>当|r|=1时，表示两变量为完全线性相关，即为函数关系。</li>
<li>当r=0时，表示两变量间<strong>无线性相关关系</strong>(不是说两个变量之间完全不相关，应该是线性不相关)</li>
<li>当0&lt;|r|&lt;1时，表示两变量存在一定程度的线性相关。且|r|越接近1，两变量间线性关系越密切；|r|越接近于0，表示两变量的线性相关越弱。</li>
<li>一般可按三级划分：|r|&lt;0.4为低度线性相关；0.4≤|r|&lt;0.7为显著性相关；0.7≤|r|&lt;1为高度线性相关。</li>
</ul>
<p>相关系数矩阵是由矩阵各列间的相关系数构成，相关矩阵第i行第j列的元素就是原矩阵第i列和第j列的相关系数。</p>
<p><strong>总结</strong>：由协方差矩阵和相关矩阵的公式定义可知，经标准化（或正态化：将原始数据处理成均值为0，方差为1 的标准数据）的样本数据的协方差矩阵就是原始样本数据的相关矩阵。</p>
<p><em>附上pandas处理多维数据，一般用来查看标签变量的统计特性和相关性</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">In [7]: df = pd.DataFrame(np.random.rand(10, 3), columns=[&apos;X1&apos;,&apos;Y1&apos;,&apos;Z3&apos;])</div><div class="line"></div><div class="line">In [8]: df</div><div class="line">Out[8]:</div><div class="line">         X1        Y1        Z3</div><div class="line">0  0.258116  0.667943  0.954830</div><div class="line">1  0.584975  0.546284  0.045745</div><div class="line">2  0.698974  0.409223  0.307409</div><div class="line">3  0.073166  0.356393  0.722233</div><div class="line">4  0.339093  0.146043  0.614686</div><div class="line">5  0.624361  0.062805  0.574546</div><div class="line">6  0.886631  0.217291  0.258432</div><div class="line">7  0.403345  0.035377  0.096881</div><div class="line">8  0.663185  0.376171  0.593964</div><div class="line">9  0.789347  0.489057  0.564688</div><div class="line"></div><div class="line">In [9]: df.describe()</div><div class="line">Out[9]:</div><div class="line">              X1         Y1         Z3</div><div class="line">count  10.000000  10.000000  10.000000</div><div class="line">mean    0.532119   0.330659   0.473341</div><div class="line">std     0.255544   0.210851   0.287745</div><div class="line">min     0.073166   0.035377   0.045745</div><div class="line">25%     0.355156   0.163855   0.270676</div><div class="line">50%     0.604668   0.366282   0.569617</div><div class="line">75%     0.690027   0.469099   0.609506</div><div class="line">max     0.886631   0.667943   0.954830</div><div class="line"></div><div class="line">In [10]: df.describe().loc[[&apos;min&apos;,&apos;max&apos;,&apos;std&apos;], &apos;Z3&apos;]</div><div class="line">Out[10]:</div><div class="line">min    0.045745</div><div class="line">max    0.954830</div><div class="line">std    0.287745</div><div class="line">Name: Z3, dtype: float64</div><div class="line"></div><div class="line">In [11]: corrdf=df.corr()#输出相关矩阵</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      关于统计学概率论的基础知识总结
    
    </summary>
    
      <category term="基础知识" scheme="calyp.github.io/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
    
    
      <category term="数学知识" scheme="calyp.github.io/tags/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>图像质量评估论文笔记</title>
    <link href="calyp.github.io/2019/02/26/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>calyp.github.io/2019/02/26/图像质量评估论文笔记/</id>
    <published>2019-02-26T00:46:12.000Z</published>
    <updated>2019-03-01T07:10:58.243Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像质量评估-IQA"><a href="#图像质量评估-IQA" class="headerlink" title="图像质量评估(IQA)"></a>图像质量评估(IQA)</h1><h2 id="Convolutional-Neural-Networks-for-No-Reference-Image-Quality-Assessment"><a href="#Convolutional-Neural-Networks-for-No-Reference-Image-Quality-Assessment" class="headerlink" title="Convolutional Neural Networks for No-Reference Image Quality Assessment"></a>Convolutional Neural Networks for No-Reference Image Quality Assessment</h2><p><strong>Paper</strong>:<a href="https://ieeexplore.ieee.org/iel7/6909096/6909393/06909620.pdf" target="_blank" rel="external">Convolutional Neural Networks for No-Reference Image Quality …</a><br><strong>code</strong>:-<br><strong>Author</strong>:Le Kang，Peng Ye，Yi Li，David Doermann<br><strong>time</strong>:2014CVPR</p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>In this work we describe a Convolutional Neural Network (CNN) to accurately predict image quality without a reference image. Taking image patches as input, the CNN works in the spatial domain without using hand-crafted features that are employed by most previous methods. The network consists of one convolutional layer with max and min pooling, two fully connected layers and an output node. Within the network structure, feature learning and regression are integrated into one optimization process, which leads to a more effective model for estimating image quality. This approach achieves state of the art performance on the LIVE dataset and shows excellent generalization ability in cross dataset experiments. Further experiments on images with local distortions demonstrate the local quality estimation ability of our CNN, which is rarely reported in previous literature.<br><strong>翻译</strong>：在这篇论文中，将会介绍如何在不需要参考图像的情况下利用CNN来准确预测图像质量(NR-IQA)。输入为图像的patch，CNN不需要利用手工特征，该网络包括一个卷积层和最大、最小池化层、两个全连接层和输出节点。在网络结构中，特征学习和回归被集成到一个优化过程中，这样模型预测图像质量会更有效。该方法在LIVE数据集上达到了state-of-the-art，并且在交叉数据集实验中泛化能力优异，对局部失真图像的进一步实验表明了CNN的局部质量评估能力，这在以前的文献中很少报道。</p>
<h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h3><ul>
<li>修改了网络结构，可以更有效的图像的质量特征，估计图像质量更准确</li>
<li>提出了一个新的框架，可以让特征学习和预测在局部区域进行，而先前的工作都是在整个图像上得到累积特征以获得用于估计总体质量的统计数据。而我们的方法可以在小的patch(比如32x32)上估计质量评分。</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本篇论文就是对灰度图像，首先执行对比度归一化，然后采样不重叠的patch，使用CNN来估计每个patch的质量得分和平均patch得分来估计图像的质量分数，CNN就是一个很简单的回归网络。需要注意的是，本文采用的不是对整体进行对比度归一化，而是在<strong>局部的patch上做对比度归一化(contract normalization)</strong>，因为在全局做平均的归一化会有性能损失。归一化不仅可以缓解早期工作中使用sigmoid神经单元引起的饱和问题，还使网络对光照和对比度变化具有鲁棒性，值得注意的是，在某些应用中将亮度和对比度的变化也看作失真，但这里主要关注的是图像质量下降（模糊、压缩和加性噪声）引起的失真。</p>
<hr>
<h2 id="Quality-Aware-Network-for-Set-to-Set-Recognition"><a href="#Quality-Aware-Network-for-Set-to-Set-Recognition" class="headerlink" title="Quality Aware Network for Set to Set Recognition"></a>Quality Aware Network for Set to Set Recognition</h2><p><strong>paper</strong>:<a href="https://arxiv.org/abs/1704.03373" target="_blank" rel="external">Quality Aware Network for Set to Set Recognition</a><br><strong>code</strong>:<a href="https://github.com/sciencefans/Quality-Aware-Network`caffe`" target="_blank" rel="external">https://github.com/sciencefans/Quality-Aware-Network`caffe`</a><br><strong>Author</strong>:  Yu Liu，JunJie Yan，Wanli Ouyang<br><strong>time</strong>: 2017CVPR</p>
<h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><p><strong>翻译</strong>:</p>
<h3 id="Contribution-1"><a href="#Contribution-1" class="headerlink" title="Contribution"></a>Contribution</h3><h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><hr>
<h2 id="Automatic-Face-Image-Quality-Prediction"><a href="#Automatic-Face-Image-Quality-Prediction" class="headerlink" title="Automatic Face Image Quality Prediction"></a>Automatic Face Image Quality Prediction</h2><p><strong>paper</strong>:<a href="https://arxiv.org/abs/1706.09887" target="_blank" rel="external">Automatic Face Image Quality Prediction</a><br><strong>code</strong>:<br><strong>Author</strong>:<br><strong>time</strong>: 2017</p>
<h3 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h3><p>This paper targets on the problem of set to set recognition, which learns the metric between two image sets. Images in each set belong to the same identity. Since images in a set can be complementary, they hopefully lead to higher accuracy in practical applications. However, the quality of each sample cannot be guaranteed, and samples with poor quality will hurt the metric. In this paper, the quality aware network (QAN) is proposed to confront this problem, where the quality of each sample can be automatically learned although such information is not explicitly provided in the training stage. The network has two branches, where the first branch extracts appearance feature embedding for each sample and the other branch predicts quality score for each sample. Features and quality scores of all samples in a set are then aggregated to generate the final feature embedding. We show that the two branches can be trained in an end-to-end manner given only the set-level identity annotation. Analysis on gradient spread of this mechanism indicates that the quality learned by the network is beneficial to set-to-set recognition and simplifies the distribution that the network needs to fit. Experiments on both face verification and person re-identification show advantages of the proposed QAN. The source code and network structure can be downloaded at GitHub </p>
<p><strong>翻译</strong>:这篇论文的目标聚焦在<strong>set-to-set</strong>识别，学习两个数据集之间的metric。每个数据集中的图像属于同一类别，因为集合中的图片都是互相补充的，所以有希望可以在实际应用中达到更高的准确率。然鹅每个样本的质量并不能保证，</p>
<h3 id="Contribution-2"><a href="#Contribution-2" class="headerlink" title="Contribution"></a>Contribution</h3><h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><hr>
<h1 id="人脸质量评估-FQA"><a href="#人脸质量评估-FQA" class="headerlink" title="人脸质量评估(FQA)"></a>人脸质量评估(FQA)</h1><h2 id="Face-Image-Quality-Assessment-Based-on-Learning-to-Rank"><a href="#Face-Image-Quality-Assessment-Based-on-Learning-to-Rank" class="headerlink" title="Face Image Quality Assessment Based on Learning to Rank"></a>Face Image Quality Assessment Based on Learning to Rank</h2><p><strong>paper</strong>:<a href="https://ieeexplore.ieee.org/iel7/97/4358004/06877651.pdf" target="_blank" rel="external">Face Image Quality Assessment Based on Learning to Rank</a><br><strong>code</strong>: 开源了<br><strong>Author</strong>:<br><strong>time</strong>: 2015年</p>
<h3 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h3><p>Face image quality is an important factor affecting the accuracy of automatic face recognition. It is usually possible for practical recognition systems to capture multiple face images from each subject. Selecting face images with high quality for recognition is a promising stratagem for improving the system performance. We propose a learning to rank based framework for assessing the face image quality. The proposed method is simple and can adapt to different recognition methods. Experimental result demonstrates its effectiveness in improving the robustness of face detection and recognition .</p>
<p><strong>翻译</strong>:</p>
<p>人脸图像质量是影响自动人脸识别准确率的重要因素。实际识别系统通常可能从每个subject捕获多个面部图像，选择具有高质量的面部图像用于识别是用于改善系统性能的一个策略，我们提出了基于<strong>learning to rank</strong>的框架来评估面部图像质量，该方法简单，可以适应不同的识别方法，实验结果证明了其在提高人脸检测和识别鲁棒性方面的有效性。</p>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ul>
<li><p><strong>图像预处理过程</strong></p>
<p>按常理来说，图片中只有人脸区域的像素点才能被用来评估面部质量好坏，所以我们首先如下图检测到人脸，然后将其resize到68x68大小。然后送入CNN中作为输入，我们使用眼角和嘴角，因为这两处的关键点可以很清晰，并包含了大部分的人脸区域。</p>
<p><img src="/images/QQ图片20190227202934.png" alt="Q图片2019022720293"></p>
<p><img src="../images/QQ图片20190227203701.png" alt="Q图片2019022720370"></p>
<p>利用上图中简单的CNN作为关键点检测网络，在LFW数据集中挑选了10000张作为训练集，剩下的作为测试集，图一中的(b)为预测的关键点。</p>
<p>对人脸区域进行归一化，并消除面内旋转，首先计算中心点C和半径r（包含了所有关键点的最小圆），然后以2.4rx4r的矩形R来代替如图一中的(c)所示，这样就可以计算矩形的角度</p>
</li>
<li><p><strong>面部质量评估</strong></p>
<p>目前主要有两种面部质量评估的办法：</p>
<ol>
<li>利用特定的性质，比如分辨率、角度、光照强度来量化面部图像质量</li>
<li>将一张面部图像来与“标准”的人脸图像进行比较，利用它们之间的差异来度量面部质量</li>
</ol>
<p>但是这两种办法都不灵活，也缺乏实用性，因为它们都没有考虑到人脸识别算法的可能的差异性，比如对于能很好处理遮挡情况的人脸识别系统，Fig1(g)要比Fig(f)更好，而对于能很好处理姿态差异的人脸识别系统，Fig(f)要比Fig(g)好。应该以相对的方式来评估面部图像质量。</p>
<p>基于上述考虑，提出一个简单灵活的基于learning to rank的方法来进行面部质量评估，大致来讲就是，比如对于一个人脸识别方法在数据集A上的表现要好于在数据集B上的表现，那么可以认为数据集A中图像质量要高于数据集B中图像的质量，然后同一数据集中的图像的质量分数应该被看做是相同的。具体公式论文中有描述，这里不详细写。但是作者利用的是显式映射函数而不是kernel SVM来计算RQS(Rank based Quality Score)</p>
<p><img src="/images/QQ图片20190227211318.png" alt="Q图片2019022721131"></p>
</li>
</ul>
<h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><p>从论文的结果上来看还是很好的，而作者开源的代码在实际应用场景中也表现得很鲁棒</p>
<p><img src="/images/QQ图片20190227212235.png" alt="Q图片2019022721223"></p>
<hr>
<blockquote>
<p>模板</p>
<h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><p><strong>paper</strong>:<br><strong>code</strong>:<br><strong>Author</strong>:<br><strong>time</strong>: </p>
<h3 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h3><p><strong>翻译</strong>:</p>
<h3 id="Contribution-3"><a href="#Contribution-3" class="headerlink" title="Contribution"></a>Contribution</h3><h3 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h3></blockquote>
]]></content>
    
    <summary type="html">
    
      关于图像质量评估（包括人脸图像质量评估）的论文笔记
    
    </summary>
    
      <category term="论文笔记" scheme="calyp.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="图像质量评估" scheme="calyp.github.io/tags/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>每日阅读</title>
    <link href="calyp.github.io/2019/02/25/%E6%AF%8F%E6%97%A5%E9%98%85%E8%AF%BB/"/>
    <id>calyp.github.io/2019/02/25/每日阅读/</id>
    <published>2019-02-25T00:46:12.000Z</published>
    <updated>2019-02-26T02:37:00.783Z</updated>
    
    <content type="html"><![CDATA[<h1 id="杂"><a href="#杂" class="headerlink" title="杂"></a>杂</h1><h2 id="Rethinking-ImageNet-Pre-training"><a href="#Rethinking-ImageNet-Pre-training" class="headerlink" title="Rethinking ImageNet Pre-training"></a>Rethinking ImageNet Pre-training</h2><p><strong>Paper</strong>:<a href="http://arxiv.org/pdf/1811.10104v1" target="_blank" rel="external">Rethinking ImageNet Pre-training</a><br><strong>code</strong>:-<br><strong>Author</strong>:Kaiming He、 Ross Girshick 、Piotr Dollar 、Facebook AI Research (FAIR)<br><strong>time</strong>:2018</p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained <strong>from random initialization</strong>.The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine -tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge.Training from random initialization is surprisingly robust; our results hold even when: (i) Using only 10% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics.Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy.To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data–a result on par with the top COCO 2017 competition results that used ImageNet pre-training.These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of ‘ pre-training and fine -tuning’ in computer vision.<br><strong>翻译</strong>：我们使用随机初始化训练的标准模型报告在COCO数据集上的物体检测和物体分割的竞争结果。即使使用baseline model的超参数，结果也不比它们的ImageNet预训练对应物差（Mask R-CNN）针对精细调整预训练模型进行了优化，唯一的例外是增加训练迭代次数，因此随机初始化模型可以收敛。随机初始化训练非常强大;我们的结果即使在以下情况下仍然存在：（i）仅使用10％的训练数据，（ii）用于更深和更宽的模型，以及（iii）用于多个任务和指标。实验表明在ImageNet预训练速度能加速训练时候的收敛速度，但不一定提供正则化或提高最终在目标任务上的准确性。为了突破这个范围，我们在不使用任何外部数据的情况下在COCO对象检测上达到了50.9 AP  - 这个结果与2017年COCO相同使用ImageNet预训练的最好的竞赛结果差不多。这些结论挑战了ImageNet与训练对于依赖性任务的传统智慧，我们期望这些发现将鼓励人们重新思考计算机视觉中的“pre-training and fine-tuneing”范式。</p>
<p><img src="/images/154392682.jpg" alt="543926824(1"></p>
<h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h3><ul>
<li>采用基于ImageNet的预训练模型参数可以加快模型的收敛速度，尤其是在训练初期，而基于随机初始化网络结构参数训练模型时在采用合适的归一化方式且迭代足够多的次数后也能够达到相同的效果</li>
</ul>
<p><img src="/images/154392682.jpg" alt="img"></p>
<ul>
<li><p>采用基于ImageNet的预训练模型参数训练得到的模型泛化能力不如基于随机初始化网络结构训练得到的模型，前者更容易出现过拟合，因此需要选择合适的超参数训练模型，从而尽可能减少过拟合风险（前提是数据量足够大，比如10K以上的COCO数据集）</p>
<p><img src="/images/QQ%E5%9B%BE%E7%89%8720181204204421.png" alt="Q图片2018120420442"></p>
</li>
<li><p>当迁移任务的目标对空间位置信息比较敏感时（比如目标检测、人体关键点检测任务），采用基于ImageNet的预训练模型并不会有太大的帮助</p>
</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><hr>
<h2 id="Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks"><a href="#Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks" class="headerlink" title="Bag of Tricks for Image Classification with Convolutional Neural Networks"></a>Bag of Tricks for Image Classification with Convolutional Neural Networks</h2><p><strong>paper</strong>:<a href="http://arxiv.org/abs/1812.01187v1" target="_blank" rel="external">Bag of Tricks for Image Classification with Convolutional Neural Networks</a><br><strong>code</strong>:-<br><strong>Author</strong>:<br><strong>time</strong>:2018</p>
<h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><p>Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example,we raise ResNet-50’s top-1 validation accuracy from 75.3% to 79.29% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.<br><strong>翻译</strong>：近期图像分类领域取得的许多进展可以归功于训练过程的改进，比如图像增强的和优化方法的改进。然而在现在的文献中，大多数的改进要么是在实现细节中简单的提及，或者仅在源码中可见。在这篇论文中，我们将研究这些改进的方法，并通过消融研究凭经验评估它们对最终模型准确性的影响，我们将展示，通过将这些改进结合在一起，能够显著改进各种CNN模型，比如，我们在ImageNet上将ResNet-50的top-1验证集准确率从75.3%提高到79.29%。我们还将证明，图像分类准确率的提高可以在其他应用领域比如对象检测和语义分割中实现更好的迁移学习性能。</p>
<h3 id="Contribution-1"><a href="#Contribution-1" class="headerlink" title="Contribution"></a>Contribution</h3><p><strong>Some tricks for training:</strong></p>
<ul>
<li>Large-batch training<ul>
<li>Linear scaling learning rate<br>使用大的batch size，可能会减慢训练速度，对于收敛来说，收敛速度随着batch size的增加而变慢，换句话说，对于相同的迭代次数，用大的batch size比小的batch size 训练的模型验证集精度要低一些。He等人选择0.1作为batch_size=256时初始学习率，所以如果对于batch_size=b的，初始学习率可以选择0.1xb/256。</li>
<li>Learning rate warmup<br>gradual warmup strategy:将学习率从0线性增加到初始学习率，比如，假设我们使用前m个batch的样本来预热，初始学习速率设为η ,那么在第i个batch的时候(0&lt;=i&lt;=m)，学习率为iη /m。</li>
</ul>
</li>
<li>Low-precision<br>神经网络一般使用FP32（float point 32)训练，所有的数据都是以FP32格式存储，然而现在新的硬件可能对更低精度的数据类型增强了算术逻辑单元（也就是说，FP16要比FP32训练更快）尽管具有性能优势，但是精度降低后，范围更窄了，使得结果更可能超出范围，然后扰乱训练进度。建议存储所有参数和激活单元用FP16，并使用FP16计算梯度，同时，所有参数在FP32中都有一个副本用于参数更新，此外，将损失乘以一个标量可以更好的对齐使用FP16的梯度范围。</li>
<li>Model tweaks<br>Model tweak是对网络体系结构的微小调整，例如改变特定卷积层的步长，这种调整通常几乎不会改变计算复杂性，但可能对模型的准确率具有不可忽略的影响。</li>
<li>Trainging refinement<ul>
<li>Cosine learning rate decay</li>
<li>Label smoothing</li>
<li>Knowledge distilling</li>
<li>Mix-up training</li>
</ul>
</li>
</ul>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><p>研究了十几个训练卷积神经网络的技巧，这些技巧包括对网络结构、数据预处理、损失函数和学习速率的调整，将所有这些堆叠在一起可以显著提高准确率。</p>
<hr>
<h2 id="Graph-Neural-Networks-A-Review-of-Methods-and-Applications"><a href="#Graph-Neural-Networks-A-Review-of-Methods-and-Applications" class="headerlink" title="Graph Neural Networks:A Review of Methods and Applications"></a>Graph Neural Networks:A Review of Methods and Applications</h2><p><strong>paper</strong>:<a href="http://arxiv.org/pdf/1812.08434v1" target="_blank" rel="external">http://arxiv.org/pdf/1812.08434v1</a><br><strong>code</strong>:-<br><strong>Author</strong>:<br><strong>time</strong>: 2018</p>
<h3 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h3><p>Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require that a model to learn from graph inputs. In other domains such as learning from non structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with an arbitrary depth. Although the primitive graph neural networks have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on graph convolutional network (GCN) and gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.<br><strong>翻译</strong>:许多学习任务需要处理包含元素之间丰富关系的图形数据，比如物理系统建模、学习分子指纹、预测蛋白质界面和分类疾病需要一个模型来学习图形输入，在其他领域，如从文本和图像等非结构性数据中学习，对提取结构的推理，如句子的依赖树和图像的场景图，是一个重要的研究课题，也需要图形推理模型。图形神经网络（GNN）是连接模型，通过图形节点之间的消息传递捕获图形的依赖性，与标准神经网络不同，图形神经网络保留一种状态，该状态可以表示来自其邻域的任意深度的信息。虽然已发现原始图神经网络难以训练固定点，但网络架构，优化技术和并行计算的最新进展使得能够成功地学习它们。近年来，基于图卷积网络（GCN）和门控图神经网络（GGNN）的系统已经在上述许多任务中展示了突破性的性能。在本次调查中，我们对现有的图神经网络模型进行了详细的回顾，系统地对应用进行了分类，并为未来的研究提出了四个未解决的问题。</p>
<h3 id="Contribution-2"><a href="#Contribution-2" class="headerlink" title="Contribution"></a>Contribution</h3><h3 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h3><hr>
<h1 id="人脸识别"><a href="#人脸识别" class="headerlink" title="人脸识别"></a>人脸识别</h1><h2 id="Git-Loss-for-Deep-Face-Recognition"><a href="#Git-Loss-for-Deep-Face-Recognition" class="headerlink" title="Git Loss for Deep Face Recognition"></a>Git Loss for Deep Face Recognition</h2><p><strong>paper</strong>:<a href="https://arxiv.org/abs/1807.08512" target="_blank" rel="external">Git Loss for Deep Face Recognition</a><br><strong>code</strong>:<a href="https://github.com/kjanjua26/Git-Loss-For-Deep-Face-Recognition" target="_blank" rel="external">https://github.com/kjanjua26/Git-Loss-For-Deep-Face-Recognition</a><br><strong>Author</strong>:<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Calefati%2C+A" target="_blank" rel="external">Alessandro Calefati</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Janjua%2C+M+K" target="_blank" rel="external">Muhammad Kamran Janjua</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nawaz%2C+S" target="_blank" rel="external">Shah Nawaz</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gallo%2C+I" target="_blank" rel="external">Ignazio Gallo</a><br><strong>time:</strong>2018</p>
<h3 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h3><p>Convolutional Neural Networks (CNNs) have been widely used in computer vision tasks, such as face recognition and verification, and have achieved state-of-the-art results due to their ability to capture discriminative deep features. Conventionally, CNNs have been trained with softmax as supervision signal to penalize the classification loss. In order to further enhance discriminative capability of deep features, we introduce a joint supervision signal, Git loss, which leverages on softmax and center loss functions. The aim of our loss function is to minimize the intra-class variations as well as maximize the inter-class distances. Such minimization and maximization of deep features is considered ideal for face recognition task. We perform experiments on two popular face recognition benchmarks datasets and show that our proposed loss function achieves maximum separability between deep face features of different identities and achieves state-of-the-art accuracy on two major face recognition benchmark datasets: Labeled Faces in the Wild (LFW) and YouTube Faces (YTF). However, it should be noted that the major objective of Git loss is to achieve maximum separability between deep features of divergent identities. The code has also been made publicly available<br><strong>翻译</strong>：卷积神经网络（CNN）已被广泛用于计算机视觉任务，例如人脸识别和验证，并且由于其捕获有辨别力的深层特征的能力而已经获得了最先进的结果。传统上，CNN已经用softmax作为监督信号进行训练以惩罚分类损失。为了进一步增强深度特征的判别能力，我们引入了一个联合监督信号Git loss，它利用softmax和center loss函数。我们的损失函数的目标是最小化类内距离以及最大化类间距离。这种深度特征的最小化和最大化被认为是面部识别任务的理想选择。我们在两个流行的人脸识别benchmark上进行实验，并表明我们提出的损失函数实现了不同身份的深面特征之间的最大可分离性，并在两个主要的人脸识别基准数据集上实现了最先进的准确度：LFW和YouTube Faces（YTF）。但是，应该指出的是，Git loss的主要目标是在不同身份的深层特征之间实现最大可分离性。该代码也已公开发布。</p>
<p><img src="/images/QQ图片20181208205830.png" alt="Q图片2018120820583"></p>
<h3 id="Contribution-3"><a href="#Contribution-3" class="headerlink" title="Contribution"></a>Contribution</h3><ul>
<li>A novel loss function which leverages on softmax and center loss to provide segregative abilities to deep architectures and enhance the discrimination of deep features tofurther improve the face recognition task</li>
<li>Easy implementation of the proposed loss function with standard CNN architectures.Our network is end-to-end trainable and can be directly optimized by fairly standard optimizers such as Stochastic Gradient Descent (SGD).</li>
<li>We validate our ideas and compare Git loss against different supervision signals. We evaluate the proposed loss function on available datasets, and demonstrate state-ofthe-art results. </li>
</ul>
<h3 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h3><p>在center loss的基础上增加了额外的约束，使学习到的特征比center loss具有更大的类间距离和更小的类内距离。</p>
<hr>
<h1 id="表情识别"><a href="#表情识别" class="headerlink" title="表情识别"></a>表情识别</h1><h2 id="Facial-Expression-Recognition-with-Inconsistently-Annotated-Datasets"><a href="#Facial-Expression-Recognition-with-Inconsistently-Annotated-Datasets" class="headerlink" title="Facial Expression Recognition with Inconsistently Annotated Datasets"></a>Facial Expression Recognition with Inconsistently Annotated Datasets</h2><p><strong>paper</strong>:<a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Jiabei_Zeng_Facial_Expression_Recognition_ECCV_2018_paper.pdf" target="_blank" rel="external">http://openaccess.thecvf.com/content_ECCV_2018/papers/Jiabei_Zeng_Facial_Expression_Recognition_ECCV_2018_paper.pdf</a><br><strong>code</strong>:<a href="https://github.com/dualplus/LTNet" target="_blank" rel="external">https://github.com/dualplus/LTNet</a><br><strong>Author</strong>:  Jiabei Zeng, Shiguang Shan, and Xilin Chen<br><strong>time</strong>: ECCV2018</p>
<h3 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h3><p>Annotation errors and bias are inevitable among different facial expression datasets due to the subjectiveness of annotating facial expressions. Ascribe to the inconsistent annotations, performance of existing facial expression recognition (FER) methods cannot keep improving when the training set is enlarged by merging multiple datasets. To address the inconsistency, we propose an Inconsistent Pseudo Annotations to Latent Truth(IPA2LT) framework to train a FER model from multiple inconsistently labeled datasets and large scale unlabeled data. In IPA2LT, we assign each sample more than one labels with human annotations or model predictions. Then, we propose an end-to-end LTNet with a scheme of discovering the latent truth from the inconsistent pseudo labels and the input face images. To our knowledge, IPA2LT serves as the first work to solve the training problem with inconsistently labeled FER datasets. Experiments on synthetic data validate the effectiveness of the proposed method in learning from inconsistent labels. We also conduct extensive experiments in FER and show that our method outperforms other state-of-the-art and optional methods under a rigorous evaluation protocol involving 7 FER datasets<br><strong>翻译</strong>:由于标注面部表情的主观性，标注错误和误差在不同的面部表情数据集种是不可避免的，归因于不一致的标注。当通过合并多个数据集扩大数据集时，现有的面部表情识别方法的性能不能持续提升，为了解决这种不一致性，我们提出了一种不一致的潜在真实伪标注(IPA2LT)框架，用于从多个不一致标记的数据集和大规模未标记数据中训练FER模型。在IPA2LT中，我们未每个样本分配多个带有人工标注或模型预测的标签，然后我们提出一种端到端的LTNet，其思想时从不一致的伪标签和输入的面部图像中发现潜在的真相(也就是真实标签)。据我们所知，IPA2LT是第一个用不一致标记的FER数据集来解决训练问题的工作，对合成数据的实验验证了所提出的方法从不一致标签中学习的有效性。我们还在FER中进行了大量实验，并表明我们的方法在涉及7个FER数据集的严格评估协议下优于其他最先进和可选的方法。</p>
<p><img src="/images/QQ%E5%9B%BE%E7%89%8720181218105946.png" alt="Q图片2018121810594"></p>
<p>训练过程分为3步：</p>
<ol>
<li>分别从数据集A和B中训练两个模型标注工具</li>
<li>模型的预测结果可能和人工标注不一致，它们将作为一些人工标注数据集和大规模未标注数据集中每个图片的多个标签，每个未标注数据作为数据集A和数据集B之间的桥梁，因为它们是用相同的模型预测出来的结果。</li>
<li>训练LTNet，预测图片输出隐藏的真实标签，</li>
</ol>
<h3 id="Contribution-4"><a href="#Contribution-4" class="headerlink" title="Contribution"></a>Contribution</h3><ul>
<li>提出了一个相对未研究过的问题：如何从多个标注不同的数据集中训练分类器，据我们所知，这是第一个解决不同的FER数据集标注不一致的工作</li>
<li>提出了IPA2LT的框架从多个标注不一致的数据集和大规模未标注的数据上训练一个FER模型，提出了一个端到端训练的LTNet</li>
<li>在合成数据和真实数据上的实验验证了所提出的方法在从不一致标签中学习的有效性，我们在FER中进行了广泛的实验，并在涉及7个FER数据集的严格评估写一下展示了IPA2LT优于现有技术的优势</li>
</ul>
<h3 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h3><p>从表情识别这一任务的特性–具有主观性，发现数据集标签不一致的情况，提出一个端到端的训练模型，从不一致的标签学习到标签之间潜在的关联，从而输出隐藏的真实标签，对于合并数据集来说，是一种方法，且有一定的合理性，并进行大量实验，发现在表情数据集上合并训练的效果要优于单独训练，并且本文提出的办法要优于其他混合的方法。</p>
<hr>
<h2 id="Facial-Expression-Recognition-using-Facial-Landmark-Detection-and-Feature-Extraction-on-Neural-Networks"><a href="#Facial-Expression-Recognition-using-Facial-Landmark-Detection-and-Feature-Extraction-on-Neural-Networks" class="headerlink" title="Facial Expression Recognition using Facial Landmark Detection and Feature Extraction on Neural Networks"></a>Facial Expression Recognition using Facial Landmark Detection and Feature Extraction on Neural Networks</h2><p><strong>paper</strong>:<a href="http://arxiv.org/abs/1812.04510v1" target="_blank" rel="external">Facial Expression Recognition using Facial Landmark Detection and Feature Extraction on Neural Networks</a><br><strong>code</strong>:<br><strong>Author</strong>:  Fuzail Khan<br><strong>time</strong>: 2018.12</p>
<h3 id="Abstract-5"><a href="#Abstract-5" class="headerlink" title="Abstract"></a>Abstract</h3><p>The proposed framework in this paper has the primary objective of classifying the facial expression shown by a person using facial landmark detection and feature extraction. These classifiable expressions can be any one of the six universal emotions along with the neutral emotion. After initial facial detection, facial landmark detection and feature extraction are performed (where in the landmarks were determined to be the fiducial features: theeyebrows, eyes, nose and lips). This is primarily done using the Sobel horizontal edge detection method and the Shi Tomasi corner point detector. Thisleads to input feature vectors being formulated and trained into a Multi-LayerPerceptron (MLP) neural network in order to classify the expression beingdisplayed. Facial Expression Recognition (FER) is a significant step inreaching the eventual goal of artificial intelligence. If efficient methods can be brought about to automatically recognize these expressions, major advances may be achieved in computer vision.<br><strong>翻译</strong>:本文提出的框架的主要目的是使用人脸关键点检测和特征提取对面部表情进行分类，这些可分类的表情可以是6种普遍情绪种的任何一种或者中性情绪，在初始的面部检测之后，执行面部关键点检测和特征提取，主要使用Sobel水平边缘检测方法和Shi Tomasi角点检测器完成。这导致输入特征向量被指定并训练成多层感知器(MLP)神经网络，以便对表情进行分类。FER是实现人工智能最终目标的重要一步，如果可以利用有效的方法自动识别这些表情，则可以在计算机视觉中实现重大进步。</p>
<h3 id="Contribution-5"><a href="#Contribution-5" class="headerlink" title="Contribution"></a>Contribution</h3><p><img src="/images/QQ%E5%9B%BE%E7%89%8720181222194912.png" alt="Q图片2018122219491"><br><img src="/images/QQ%E5%9B%BE%E7%89%8720181222195057.png" alt="Q图片2018122219505"></p>
<ul>
<li>首先对输入的图片进行预处理，主要是去除不想要的噪声，增强图片的对比度，先用低通3x3高斯滤波器，有助于平滑图像并使梯度强度正则化，然后进行对比度自适应直方图均衡，用于光照强度修正</li>
<li>人脸检测利用基于Haar特征的级联分类器</li>
<li>关键点检测<ul>
<li>人眼检测，主要是得到左眼和右眼的RoI，然后用于提取特征点</li>
<li>鼻子检测</li>
<li>眉毛和嘴唇检测</li>
</ul>
</li>
<li>提取特征向量，通过上一步骤提取到了特征点(F1-F18)，这一步使决定输入的特征向量送入神经网络中<br><img src="/images/QQ%E5%9B%BE%E7%89%8720181222203519.png" alt="Q图片2018122220351"></li>
<li>训练神经网络，进行分类</li>
</ul>
<h3 id="Summary-5"><a href="#Summary-5" class="headerlink" title="Summary"></a>Summary</h3><p>这篇论文感觉方法还不是很先进，但是可能在某些特殊的数据集（表情很明显，图像质量比较好）效果会比较好，而论文选用的数据集从给出的例子来看，表情都是比较夸张的。</p>
<hr>
<h1 id="步态识别"><a href="#步态识别" class="headerlink" title="步态识别"></a>步态识别</h1><h2 id="GaitSet"><a href="#GaitSet" class="headerlink" title="GaitSet"></a>GaitSet</h2><p><strong>paper</strong>:<a href="https://arxiv.org/abs/1811.06186" target="_blank" rel="external">GaitSet:Regarding Gait as a Set for Cross-View Gait Recognition</a><br><strong>code</strong>:<a href="https://github.com/AbnerHqC/GaitSet" target="_blank" rel="external">https://github.com/AbnerHqC/GaitSet</a><br><strong>Author</strong>: Chao, Hanqing and He, Yiwei and Zhang, Junping and Feng, Jianfeng<br><strong>time</strong>: 2019</p>
<h3 id="Abstract-6"><a href="#Abstract-6" class="headerlink" title="Abstract"></a>Abstract</h3><p>As a unique biometric feature that can be recognized at a distance, gait has broad applications in crime prevention, forensic identification and social security. To portray a gait, existing gait recognition methods utilize either a gait template, where temporal information is hard to preserve, or a gait sequence, which must keep unnecessary sequential constraints and thus loses the flexibility of gait recognition. In this paper we present a novel perspective, where a gait is regarded as a set consisting of independent frames. We propose a new network named GaitSet to learn identity information from the set. Based on the set perspective, our method is immune to permutation of frames, and can naturally integrate frames from different videos which have been filmed under different scenarios, such as diverse viewing angles, different clothes/carrying conditions. Experiments show that under normal walking conditions, our single-model method achieves an average rank-1 accuracy of 95.0% on the CASIAB gait dataset and an 87.1% accuracy on the OU MVLP gait dataset. These results represent new state-of-the-art recognition accuracy. On various complex scenarios, our model exhibits a significant level of robustness. It achieves accuracies of 87.2% and 70.4% on CASIA-B under bag carrying and coat-wearing walking conditions, respectively. These outperform the existing best methods by a large margin. The method presented can also achieve a satisfactory accuracy with a small number of frames in a test sample, e.g., 82.5% on CASIAB with only 7 frames.<br><strong>翻译</strong>:作为一种可以远距离识别的独特的生物识别特征，步态在预防犯罪、法医鉴定和社会安全方面具有广泛应用。为了描绘步态，现有的步态识别方法利用步态模板（其中时间信息难以保存）或步态序列，其必须保持不必要的顺序约束并因此丧失步态识别的灵活性，在本文中，我们提出了一种新颖的视角，其中步态被视为由独立框架组成的集合。我们提出了一个名为GaitSet的新网络来学习集合中的身份信息。基于设定的视角，我们的方法不受帧的排列的影响，并且可以<strong>自然地整合来自在不同场景下拍摄地不同视频地帧，例如不同的视角，不同的衣服/着装</strong>。实验表明，在正常步行条件下，我们的单模型方法在CASIA-B步态数据集上实现了95%的平均Rank-1的准确率，在OU MVLP步态数据集上达到了87.1%的准确率。这些结果代表了新的最先进的步态识别准确率，在各种复杂场景中，我们的模型具有显著的鲁棒性，在携带包和穿着外套的行走场景中，CASIA-B的准确率分别达到了87.2%和70.4%的准确率，它们大大优于现有的最佳方法，本文提出的方法还在具有少量帧的测试样本下实现了令人满意的准确率，例如只有7帧的CASIA-B数据集达到了82.5%的准确率。</p>
<h3 id="Contribution-6"><a href="#Contribution-6" class="headerlink" title="Contribution"></a>Contribution</h3><h3 id="Summary-6"><a href="#Summary-6" class="headerlink" title="Summary"></a>Summary</h3><hr>
<h1 id="神经网络和森林"><a href="#神经网络和森林" class="headerlink" title="神经网络和森林"></a>神经网络和森林</h1><h2 id="Deep-Neural-Decision-Forests"><a href="#Deep-Neural-Decision-Forests" class="headerlink" title="Deep Neural Decision  Forests"></a>Deep Neural Decision  Forests</h2><p><strong>paper</strong>:<a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf" target="_blank" rel="external">Deep neural decision forests</a><br><strong>code</strong>:<br><strong>Author</strong>:<br><strong>time:</strong>ICCV2015</p>
<h3 id="Abstract-7"><a href="#Abstract-7" class="headerlink" title="Abstract"></a>Abstract</h3><p>We present Deep Neural Decision Forests – a novel approach that unifies classification trees with the representation learning  known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find onpar or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84%/6.38% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67% error obtained by the best GoogLeNet architecture (7 models, 144 crops).<br><strong>翻译</strong>:我们提出了深度神经决策森林–一个新的方法可以通过端到端的训练将分类树和深度卷积卷积网络中的表征学习功能统一起来。为了把这两个方法结合起来，我们引入了一个随机可微分的决策树模型，控制一般在（深度）卷积网络的初始层进行的表征学习。我们的模型不同于传统的深度网络，因为决策森林进行最后的预测，它与传统的决策森林不同，因为我们提出了分裂节点和叶子节点参数的联合全局优化原则。我们展示了在类似MNIST和ImageNet的benchmark上的实验结果，并与最先进的深度模型进行比较，发现了优于它们的结果。最值得注意的是，当我们在ImageNet验证数据上获得的Top5误差仅为7.84%/6.38%。因此即使没有任何形式的训练数据增强，我们也在最佳的GoogleNet架构(7种模型，144 crops）上得到6.67%的误差。</p>
<p><img src="/images/QQ%E5%9B%BE%E7%89%8720181204203113.png" alt="Q图片2018120420311"></p>
<h3 id="Contribution-7"><a href="#Contribution-7" class="headerlink" title="Contribution"></a>Contribution</h3><p>论文提出将分类树模型和深度神经网络的特征学习相结合进行端到端训练的深度学习方法。该方法使用决策森林(decision forest)作为最终的预测模型，提出了一套完整的、联合的、全局的深度学习参数优化方法。在手写数据库MNIST和图像分类数据库ImageNet的实验中都取得了超越当前最好方法的结果。</p>
<h3 id="Summary-7"><a href="#Summary-7" class="headerlink" title="Summary"></a>Summary</h3><p>和年龄估计的deep regression forest不同的是，这是针对分类任务的决策森林，DRF正是受这篇论文的启发，将分类转化为回归，这也是年龄任务的特点。</p>
<hr>
<h2 id="Distilling-a-Neural-Network-Into-a-Soft-Decision-Tree"><a href="#Distilling-a-Neural-Network-Into-a-Soft-Decision-Tree" class="headerlink" title="Distilling a Neural Network Into a Soft Decision Tree"></a>Distilling a Neural Network Into a Soft Decision Tree</h2><p><strong>paper</strong>:<a href="https://arxiv.org/abs/1711.09784" target="_blank" rel="external">Distilling a neural network into a soft decision tree</a><br><strong>code</strong>:<br><strong>Author</strong>:Geoffrey Hinton 、Nicholas Frosst、Google Brain Team<br><strong>time:</strong>2017</p>
<h3 id="Abstract-8"><a href="#Abstract-8" class="headerlink" title="Abstract"></a>Abstract</h3><p>Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large [Szegedy et al., 2015, Wu et al., 2016, Jozefowicz et al., 2016, Graves et al., 2013]. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.<br><strong>翻译</strong>：深度神经网络已经被证明是执行分类任务非常有效的方法，当输入数据是高维、输入和输出之间的关系很复杂、标记的训练样本数量很大时，表现优异。但很难解释为什么学习型网络会针对特定测试用例做出特定的分类觉得。这是因为它们依赖分布式层次表示(distributed hierarchical representations)。如果我们可以获取神经网络获得的知识并在依赖层级决策的模型中表现同样的知识，那么解释特定决策将更容易。我们描述了一种使用训练的神经网络来创建一种<strong>软决策树</strong>的方法，该决策树比直接从训练数据中学习有更强的泛化能力。</p>
<p><img src="/images/QQ%E5%9B%BE%E7%89%8720181208183701.png" alt="Q图片2018120818370"><br>解释：上图为MNIST上训练的深度为4的软决策树的可视化图。内部节点处的图像是已学习的过滤器，而叶子处的图像是对已学习的类别概率分布的可视化。每片叶子最终最可能的分类，以及每个边缘的可能分类都被注释了。以最右边的内部节点为例，可以看到，在树的层级上，潜在的分类只有3或8，因此，学习的过滤器只是简单地学会区分这两个数字。其结果是一个查找连接3的末端的两个区域的过滤器，输出一个8。</p>
<h3 id="Contribution-8"><a href="#Contribution-8" class="headerlink" title="Contribution"></a>Contribution</h3><p>针对深度神经网络无法解释其具体决策的问题，提出<code>软决策树</code>（Soft decision tree)，相较于从训练数据中直接学习的决策树，软决策树的泛化能力更强，并通过层级决策模型把DNN所习得的知识表达出来。</p>
<h3 id="Summary-8"><a href="#Summary-8" class="headerlink" title="Summary"></a>Summary</h3><p>这篇论文和Deep neural decision forest不一样的是，这篇论文的重点是蒸馏(distillation)，从DNN蒸馏出DT(decision tree)，决策树去distilling 神经网络中的 knowledge，然后拿决策树去解释神经网络，而上一篇论文是将DNN和DT结合起来；上一篇论文是通过卷积神经网络得到原始数据的新特征放入决策树，本文是直接拿原始数据放入决策树。</p>
<hr>
<h2 id="Deep-Neural-Decision-Trees"><a href="#Deep-Neural-Decision-Trees" class="headerlink" title="Deep Neural Decision Trees"></a>Deep Neural Decision Trees</h2><p><strong>paper</strong>:<a href="http://arxiv.org/abs/1812.01187v1" target="_blank" rel="external"><a href="https://arxiv.org/abs/1806.06988" target="_blank" rel="external">Deep Neural Decision Trees</a></a><br><strong>code</strong>:<a href="https://github.com/wOOL/DNDT" target="_blank" rel="external">https://github.com/wOOL/DNDT</a><br><strong>Author</strong>:Yongxin Yang, Irene Garcia Morillo, Timothy M. Hospedales<br><strong>time</strong>:WHI 2018</p>
<h3 id="Abstract-9"><a href="#Abstract-9" class="headerlink" title="Abstract"></a>Abstract</h3><p>Deep neural networks have been proven powerful at processing perceptual data, such as images and audio. However for tabular data, tree-based models are more popular. A nice property of tree-based models is their natural interpretability. In this work, we present Deep Neural Decision Trees (DNDT) – tree models realised by neural networks. A DNDT is intrinsically interpretable, as it is a tree. Yet as it is also a neural network (NN), it can be easily implemented in NN toolkits, and trained with gradient descent rather than greedy splitting. We evaluate DNDT on several tabular datasets, verify its efficacy, and investigate similarities and differences between DNDT and vanilla decision trees. Interestingly, DNDT self-prunes at both split and feature-level.<br><strong>翻译</strong>：已经证明了深度神经网络在处理感知数据（例如图像和音频）方面是强大的。但是对于表格数据，基于树的模型应用更多，基于树的模型的一个很好特性是它们的自然可解释性，在这项工作中，我们提出了深度神经决策树（DNDT）–通过神经网络实现的树模型网络。DNDT本质是可解释的，因为它是一棵树，然而，因为它也是一个神经网络，可以很容易地在NN toolkit中实现，并且使用梯度下降训练而不是贪婪地分裂。我们评估了几个DNDT地表格数据集，验证其功效，并研究DNDT之间地相似性和差异性，有趣地是，DNDT的分裂和特征级别地自我修剪。</p>
<p><img src="/images/QQ%E5%9B%BE%E7%89%8720181213111619.png" alt="Q图片2018121311161"><br>红色表示训练时的变量，黑色表示常量</p>
<p><img src="/images/QQ%E5%9B%BE%E7%89%8720181213111634.png" alt="Q图片2018121311163"><br>相同的网络用传统决策树展示，分数表示随机选择的6个事例的分类路线。</p>
<h3 id="Contribution-9"><a href="#Contribution-9" class="headerlink" title="Contribution"></a>Contribution</h3><h3 id="Summary-9"><a href="#Summary-9" class="headerlink" title="Summary"></a>Summary</h3><p>和deep neural decision forest(Deep-NDF)不同的方面有：</p>
<ol>
<li>DNDT不是使用交替优化策略优化结构学习（splitting)和参数学习(score matrix)，相反，直接使用反向传播来学习它们</li>
<li>不会将分割限制为二进制（左或右），因为我们应用可以将节点分成多个（&gt;=2)叶子的可微分的bining functioin</li>
<li>我们专门为可解释性设计了我们的模型，特别是对表格数据的应用，我们可以解释每个输入特征，相比之下，NDF种的模型被设计用于预测性能并应用于原始图像数据，一些设计决策使它们不太适用于表格数据</li>
</ol>
<hr>
<blockquote>
<p>模板</p>
<h2 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h2><p><strong>paper</strong>:<br><strong>code</strong>:<br><strong>Author</strong>:<br><strong>time</strong>: </p>
<h3 id="Abstract-10"><a href="#Abstract-10" class="headerlink" title="Abstract"></a>Abstract</h3><p><strong>翻译</strong>:</p>
<h3 id="Contribution-10"><a href="#Contribution-10" class="headerlink" title="Contribution"></a>Contribution</h3><h3 id="Summary-10"><a href="#Summary-10" class="headerlink" title="Summary"></a>Summary</h3></blockquote>
]]></content>
    
    <summary type="html">
    
      平时阅读的论文笔记，比较杂，没有分类
    
    </summary>
    
      <category term="论文笔记" scheme="calyp.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="每日阅读" scheme="calyp.github.io/tags/%E6%AF%8F%E6%97%A5%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>人脸质量评估</title>
    <link href="calyp.github.io/2019/02/23/%E4%BA%BA%E8%84%B8%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0/"/>
    <id>calyp.github.io/2019/02/23/人脸质量评估/</id>
    <published>2019-02-23T12:16:20.000Z</published>
    <updated>2019-02-23T12:23:21.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="数据标注"><a href="#数据标注" class="headerlink" title="数据标注"></a>数据标注</h3><p>利用2015年的一篇人脸图像质量评估论文<a href="https://ieeexplore.ieee.org/iel7/97/4358004/06877651.pdf" target="_blank" rel="external">Face Image Quality Assessment Based on Learning to Rank</a>提供的工具对数据图像数据集进行标注，人脸图像可以利用MTCNN 进行预处理，这里将人脸上下扩展了1/2，左右扩展了1/3，不进行对齐(不改变人脸的偏转角，因为这也属于图像质量评估的一个考察因素），该工具输出的值为0-100的整数值，此处将其归一化为[0,1]之间的实数值</p>
<h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><p>在做回归问题时，特别是标签为浮点数时，最好不要使用lmdb数据格式，原始的caffe中lmdb好像对float支持不太友好，可以通过修改源码，参考链接：<a href="https://blog.csdn.net/change_things/article/details/74786185" target="_blank" rel="external">如何将float类型的特征数据存入lmdb</a>，但是此处可以换一个办法，利用hdf5存储，具体细节就不多讲了，但是有一个点要注意就是在读取数据的时候，</p>
<blockquote>
<p>caffe.io_load_image:以RGB存储，[0-1]<br>matplotlib.image.imread:以RGB存储，[0,255]<br>cv2.imread:以BGR存储，[0,255]</p>
</blockquote>
<p>所以要主要在训练模型和测试的时候要保持一致，减不减MeanValue，还有要不要归一化到[0,1]区间或者[0,255]区间</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>首先这是一个质量分数回归问题，所以一个思路就是训练一个端到端的网络，首先是一个basemodel，这里采用resnet比较合适，将最后一层的输出直接改为1，为了使模型收敛的更快，可以现在FR任务上预训练一个模型，然后再在回归任务上进行finetune，这样做有一个好处，就是之前NormFace论文还是哪篇论文中提到了图像质量越好，提取到的特征模长越大，所以这个预训练应该还是有所帮助的。最后一个全连接层输出为1，然后以EuclideanLoss作为损失函数，进行训练，以SGD作为优化算法，最后训练得到了一个比较好的模型。</p>
<h2 id="几点反思和记录"><a href="#几点反思和记录" class="headerlink" title="几点反思和记录"></a>几点反思和记录</h2><ul>
<li><p>对于标签为浮点数的数据集，最好用h5存储，存储时一定要注意数据存储的格式要一致</p>
</li>
<li><p>之前采用VGG作为basemodel，然后最后全连接层输出为1，接sigmoid，然后用EuclideanLoss作为监督信号，但是输出全部是0.5左右，本来以为是sigmoid的问题，因为当输入非常大或者非常小的时候，神经元的梯度是接近于0的，这一点特性不太好，其实只要最后一个全连接层接损失函数就好，只要迭代次数够多，最终全连接层的输出会慢慢收敛的，所以没必要硬加一个约束，可能还会起到不好的效果。但是VGG16的效果并不好，无论训练多久，其输出一直在0.5附近，经过分析，应该是发生了<strong>梯度弥散</strong>，可以加BatchNorm层，于是自己设计了一个浅层的模型做实验，发现最后的结果也非常好，和经过预训练的resnet18效果差不多，但是模型小多了，这说明可能这个简单的回归任务，只要参数和数据适配，训练参数设置合理，网络结构没有太大差别。</p>
<p><img src="/images/QQ%E5%9B%BE%E7%89%8720190223195039.png" alt="Q图片2019022319503"></p>
</li>
</ul>
<ul>
<li>至于优化算法，有地方说caffe的SGD不太适合作为回归任务的优化算法，但是之前训练一直不收敛，是由于数据存储的问题，改用adam之后也没有太大差，所以SGD还是可以作为回归的优化算法的</li>
<li><strong>BatchNorm是个好东西</strong></li>
</ul>
]]></content>
    
    <summary type="html">
    
      人脸图像质量评估
    
    </summary>
    
      <category term="深度学习" scheme="calyp.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="图像质量评估，caffe" scheme="calyp.github.io/tags/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0%EF%BC%8Ccaffe/"/>
    
      <category term="回归" scheme="calyp.github.io/tags/%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>年龄估计</title>
    <link href="calyp.github.io/2018/12/05/%E5%B9%B4%E9%BE%84%E4%BC%B0%E8%AE%A1/"/>
    <id>calyp.github.io/2018/12/05/年龄估计/</id>
    <published>2018-12-05T08:48:48.000Z</published>
    <updated>2019-02-26T09:03:08.993Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><ul>
<li><strong>FG-Net</strong>:1002 images,0到69岁，82个人</li>
<li><strong>MORPH1</strong>:1690 images</li>
<li><strong>MORPH2</strong>:55608 images、unbalanced ethnic(96% African and European ,less than 1% Asian)，16到77岁</li>
<li><strong>AFAD</strong>:160K Asian facial images(unrelease)</li>
</ul>
<h1 id="Ordinal-Regression"><a href="#Ordinal-Regression" class="headerlink" title="Ordinal Regression"></a>Ordinal Regression</h1><p><strong>paper：</strong><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Niu_Ordinal_Regression_With_CVPR_2016_paper.pdf" target="_blank" rel="external">Ordinal Regression With Multiple Output CNN for Age Estimation</a><code>CVPR2016</code><br><strong>code:</strong><a href="https://github.com/luoyetx/OrdinalRegression" target="_blank" rel="external">Ordinal Regression</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>To address the non-stationary property of aging patterns, age estimation can be cast as an ordinal regression problem.However, the processes of extracting features and learning a regression model are often separated and optimized independently in previous work. In this paper, we propose an End-to-End learning approach to address ordinal regression problems using deep Convolutional Neural Network, which could simultaneously conduct feature learning andregression modeling. In particular, an ordinal regression problem is transformed into a series of binary classification sub-problems. And we propose a multiple output CNN learning algorithm to collectively solve these classification sub-problems, so that the correlation between these tasks could be explored. In addition, we publish an Asian Face Age Dataset (AFAD) containing more than 160K facial images with precise age ground-truths, which is the largest public age dataset to date. To the best of our knowledge, this is the first work to address ordinal regression problems by using CNN, and achieves the state-of-the-art performance on both the MORPH and AFAD datasets<br><strong>翻译</strong>：为了解决衰老模式的非平稳特性，可以将年龄估计作为序数回归问题。然而，在以前的工作中，提取特征和学习回归模型的过程通常是独立分离和优化的。在本文中，我们提出了一种端到端学习方法，使用深度卷积神经网络解决序数回归问题，可以同时进行特征学习和回归建模。特别是，序数回归问题转化为一系列二元分类子问题。我们提出了一种多输出CNN学习算法来集体解决这些分类子问题，从而可以探索这些任务之间的相关性。此外，我们发布了一个亚洲面部年龄数据集（AFAD），其中包含超过<strong>160K</strong>的面部图像，具有精确的年龄真实性，这是迄今为止最大的公共年龄数据集。据我们所知，这是通过使用CNN解决序数回归问题的第一项工作，并在MORPH和AFAD数据集上实现了最先进的性能。<br><img src="/images/QQ%E5%9B%BE%E7%89%8720181202120825.png" alt="Q图片2018120212082"></p>
<h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ul>
<li>利用端到端的深度学习方法解决序数回归问题</li>
<li>released a dataset named AFAD , the largest public dataset to date</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Rank的思想比较好，针对有序的问题，将序数回归转化为一系列的二分类问题，但是这篇论文并不是第一个使用ranking思想的论文，和以前的方法（提取手工特征或者深度学习特征，然后用SVM/SVR的分类器作为二分类器）不同的是，将ranking和CNN结合起来，可以端到端的训练。</p>
<hr>
<h1 id="RankingCNN"><a href="#RankingCNN" class="headerlink" title="RankingCNN"></a>RankingCNN</h1><p><strong>paper</strong>：<a href="http://openaccess.thecvf.com/content_cvpr_2017/poster/2148_POSTER.pdf" target="_blank" rel="external">Using Ranking-CNN for Age Estimation</a><code>CVPR2017</code><br><strong>code:</strong> <a href="https://github.com/RankingCNN/Using-Ranking-CNN-for-Age-Estimation" target="_blank" rel="external">Using-Ranking-CNN-for-Age-Estimation</a></p>
<h2 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h2><p>Human age is considered an important biometric trait for human identification or search. Recent research shows that the aging features deeply learned from large-scale data lead to significant performance improvement on facial image-based age estimation. However, age-related ordinal information is totally ignored in these approaches. In this paper, we propose a novel Convolutional Neural Network (CNN)-based framework, ranking-CNN, for age estimation. Ranking-CNN contains a series of basic CNNs, each of<br>which is trained with ordinal age labels. Then, their binary outputs are aggregated for the final age prediction. We theoretically obtain a much tighter error bound for rankingbased age estimation. Moreover, we rigorously prove that ranking-CNN is more likely to get smaller estimation errors when compared with multi-class classification approaches. Through extensive experiments, we show that statistically, ranking-CNN significantly outperforms other state-of-theart age estimation models on benchmark datasets .<br><strong>翻译</strong>:<br>人类年龄被认为是一个重要的生物特征，对人类的识别或搜索。最近的研究表明，从大规模数据中深入学习到的年龄特征使得基于人脸图像的年龄估计的性能有了显着的提高。然而，在这些方法中，与年龄相关的顺序信息被完全忽略。本文提出了一种新的基于卷积神经网络(CNN)的年龄估计框架-Ranking-CNN。Ranjing-CNN包含一系列基本CNN，每个CNN都是用顺序年龄标签训练的。然后，将它们的二进制输出进行聚合，以进行最终的年龄预测。我们从理论上得到了一个更严格的基于秩的年龄估计的误差界。此外，我们严格地证明了，与多类分类方法相比，Ranking-CNN更容易获得较小的估计误差。通过大量的实验，我们表明，在统计上，Ranking-CNN在基准数据集上的性能明显优于其他最先进的年龄估计模型。<br><img src="/images/QQ%E5%9B%BE%E7%89%8720181202121159.png" alt="Q图片2018120212115"><img src="/images/QQ%E5%9B%BE%E7%89%8720181202121223.png" alt="Q图片2018120212122"></p>
<h2 id="Contributions-1"><a href="#Contributions-1" class="headerlink" title="Contributions"></a>Contributions</h2><ul>
<li>To the best of our knowledge, ranking-CNN is the first work that uses a deep ranking model for age estimation, in which binary ordinal age labels are used to train a series of basic CNNs, one for each age group.Each basic CNN in ranking-CNN can be trained using all the labeled data, leading to better performance of feature learning and also preventing overfitting.</li>
<li>We provide a much tighter error bound for age ranking than that introduced in [2], which claimed that the final ranking error is bounded by the sum of errors generated by all the classifiers. We obtain the approximation for the final ranking error that is controlled by the maximum error produced among sub-problems. From a technical perspective, the tighter error bound provides several advantages for the training of ranking-CNN.</li>
<li>We prove that ranking-CNN, by taking the ordinal relation between ages into consideration, is more likely to get smaller estimation errors when compared with multi-class classification approaches (i.e., CNNs using the softmax function). Moreover, through extensive experiments, we show that statistically, ranking-CNN significantly outperforms other state-of-the-art age estimation methods </li>
</ul>
<h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><p>这篇论文和上篇oridnal regression的思想类似，但是效果要好很多，不同的是每个二分类器都有单独的模型，所有二分类器都不共享中间层，然后聚合所有二分类器的结果得到最终的年龄估计，另外还会得到最终排序误差的近似值，该误差由子问题中产生的最大误差控制，比上一篇论文的方法误差控制更严格。</p>
<hr>
<h1 id="SSR-Net"><a href="#SSR-Net" class="headerlink" title="SSR-Net"></a>SSR-Net</h1><p><strong>paper</strong>：<a href="https://www.csie.ntu.edu.tw/~cyy/publications/papers/Yang2018SSR.pdf" target="_blank" rel="external">SSR-Net: A Compact Soft Stagewise Regression Network for Age Estimation</a><code>IJCAI2018</code><br><strong>code:</strong> <a href="https://github.com/shamangary/SSR-Net" target="_blank" rel="external">SSRNet</a></p>
<h2 id="Abstract-2"><a href="#Abstract-2" class="headerlink" title="Abstract"></a>Abstract</h2><p>This paper presents a novel CNN model called Soft Stagewise Regression Network (SSR-Net) for age estimation from a single image with a compact model size. Inspired by DEX, we address age estimation by performing multi-class classification andthen turning classification results into regression by calculating the expected values. SSR-Net takes a coarse -to-fine strategy and performs multi-class classification with multiple stages.Each stage is only responsible for refining the decision of its previous stage for more accurate age estimation. Thus, each stage performs a task with few classes and requires few neurons, greatly reducing the model size. For addressing the quantization issue introduced by grouping ages into classes, SSR-Net assigns a dynamic range to each age class by allowing it to be shifted and scaled according to the input face image. Both the multi-stage strategy and the dynamic range are incorporated into the formulation of soft stagewise regression. A novel network architecture is proposed for carrying out soft stagewise regression. The resultant SSR-Net model is very compact and takes only 0.32 MB. Despite its compact size, SSR-Net’ s performance approaches those of the state -of-the -art methods whose model sizes are often more than 1500x larger.<br><strong>翻译</strong>：本文提出了一种新的CNN模型，称为Soft stagewise regression net(SSR-Net)，使用一个很小的compact模型对单个图像进行年龄估计。受DEX的启发，我们通过执行多类分类来解决年龄估计，然后通过计算预期值将分类结果转化为回归。 SSR-Net采用粗略到精细的策略并执行多阶段的多级分类。每个阶段仅负责完善其前一阶段的决策以进行更准确的年龄估计。因此，每个阶段执行具有少量类的任务并且需要很少的神经元，从而大大减小了模型尺寸。为了解决通过将年龄分组到类中引入的量化问题，SSR-Net通过允许根据输入面部图像移位和缩放来为每个年龄类别分配动态范围。多阶段策略和动态范围都包含在软阶段回归的公式中。提出了一种新颖的网络架构，用于进行软分段回归。由此产生的SSR-Net模型非常紧凑，仅需0.32MB。尽管尺寸紧凑，但SSR-Net的性能接近于那些模型尺寸通常超过1500倍的方法。<br><img src="/images/QQ%E5%9B%BE%E7%89%8720181202120956.png" alt="Q图片2018120212095"></p>
<h2 id="Contributions-2"><a href="#Contributions-2" class="headerlink" title="Contributions"></a>Contributions</h2><p>新的年龄估计模型，平均误差绝对值为3.16，与最好的模型（当时）差距0.5岁，但参数不到其1/1000，整个模型参数仅0.3M，非常适合用于嵌入式系统。</p>
<h2 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h2><p>采用多分段的多级分类的思想，大大减少了模型的参数，在准确性和模型大小方面做了一个均衡，另外，这么小的模型做到这种准确率很厉害。</p>
<hr>
<h1 id="DeepRegressionForest"><a href="#DeepRegressionForest" class="headerlink" title="DeepRegressionForest"></a>DeepRegressionForest</h1><p><strong>paper:</strong><a href="https://arxiv.org/abs/1712.07195" target="_blank" rel="external">Deep Regression Forests for Age Estimation</a><code>CVPR2018</code><br><strong>code</strong>:<a href="https://github.com/shenwei1231/caffe-DeepRegressionForests" target="_blank" rel="external">caffe-DeepRegressionForests</a></p>
<h2 id="Abstract-3"><a href="#Abstract-3" class="headerlink" title="Abstract"></a>Abstract</h2><p>Age estimation from facial images is typically cast as a nonlinear regression problem. The main challenge of this problem is the facial feature space w.r.t. ages is heterogeneous, due to the large variation in facial appearance across different persons of the same age and the nonstationary property of aging patterns. In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation. DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with heterogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes. This joint learning follows an alternating strategy: First, by ﬁxing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by ﬁxing the split nodes, the leaf nodes are optimized by iterating a step-size free and fastconverging update rule derived from Variational Bounding. We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them.<br><strong>翻译</strong>：<br>来自面部图像的年龄估计通常被视为非线性回归问题。这个问题的主要挑战是面部特征空间w.r.t.年龄是不均匀的，这是由于同一年龄段不同人的面部外观差异很大，以及老龄化模式的非平稳性。在本文中，我们提出了深度回归森林（DRFs），一种端到端模型，用于年龄估计。DRF将分裂节点连接到卷积神经网络（CNN）的完全连接层，并通过联合学习分裂节点处的输入相关数据分区和叶节点处的数据抽象来处理异构数据。这种联合学习遵循交替策略：首先，通过固定叶节点，分裂节点以及CNN参数通过反向传播进行优化;然后，通过修剪拆分节点，通过迭代从变分边界派生的步长自由和快速收敛更新规则来优化叶节点。我们在三个标准年龄估算基准上验证了提议的DRF，并在所有这些基准上取得了最新的成果。<br><img src="/images/Q83KC%7B3A7Y%7DPVX~YU%60383RN.png" alt="83KC{3A7Y}PVX~YU`383R"></p>
<h2 id="Contributions-3"><a href="#Contributions-3" class="headerlink" title="Contributions"></a>Contributions</h2><ul>
<li>We propose Deep Regression Forests (DRFs), an endto-end model, to deal with heterogeneous data by jointly learning input-dependant data partition at split nodes and data abstraction at leaf nodes .</li>
<li>Based on Variational Bounding, the convergence of our update rule for leaf nodes in DRFs is mathematically guaranteed.</li>
<li>We apply DRFs on three standard age estimation<br>benchmarks, and achieve state-of-the-art results. </li>
</ul>
<h2 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h2><p>用深度回归森林的思想来进行年龄估计的任务，属于一种distribution learning的办法，论文中涉及到大量的概率论的知识和大量的推导公式，将年龄分布分布到多个叶子节点，最终采用随机森林和决策树的思想，对模型训练结果做融合，模型很好训练，且准确率也不错。</p>
<hr>
<h1 id="Mean-Variance-Loss"><a href="#Mean-Variance-Loss" class="headerlink" title="Mean-Variance Loss"></a>Mean-Variance Loss</h1><p><strong>paper</strong>:<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Mean-Variance_Loss_for_CVPR_2018_paper.pdf" target="_blank" rel="external">Mean-Variance Loss for Deep Age Estimation From a Face</a><code>CVPR2018</code><br><strong>code:</strong>-</p>
<h2 id="Abstract-4"><a href="#Abstract-4" class="headerlink" title="Abstract"></a>Abstract</h2><p>Age estimation has wide applications in video surveillance, social networking, and human-computer interaction. Many of the published approaches simply treat age estimation as an exact age regression problem, and thus do not leverage a distribution’s robustness in representing labels with ambiguity such as ages. In this paper, we propose a new loss function, called mean-variance loss, for robust age estimation via distribution learning. Speciﬁcally, the mean-variance loss consists of a mean loss, which penalizes difference between the mean of the estimated age distribution and the ground-truth age, and a variance loss, which penalizes the variance of the estimated age distribution to ensure a concentrated distribution. The proposed meanvariance loss and softmax loss are jointly embedded into Convolutional Neural Networks (CNNs) for age estimation. Experimental results on the FG-NET, MORPH Album II, CLAP2016, and AADB databases show that the proposed approach outperforms the state-of-the-art age estimation methods by a large margin, and generalizes well to image aesthetics assessment.1<br><strong>翻译</strong>：<br>年龄估计在视频监控，社交网络和人机交互中具有广泛的应用。许多已发表的方法仅将年龄估计视为精确的年龄回归问题，因此不会利用分布的稳健性来表示具有模糊性的标签，例如年龄。在本文中，我们提出了一种新的损失函数，称为均值 - 方差损失，用于通过分布学习进行稳健的年龄估计。具体而言，均值 - 方差损失包括平均损失，其惩罚估计年龄分布的平均值与地面实际年龄之间的差异，以及方差损失，其惩罚估计年龄分布的方差以确保集中分布。 。所提出的均值方差损失和softmax损失共同嵌入到卷积神经网络（CNN）中用于年龄估计。在FG-NET，MORPH Album II，CLAP2016和AADB数据库上的实验结果表明，所提出的方法大大超过了现有技术的年龄估计方法，并且很好地概括了图像美学评估。<br><img src="/images/QQ%E5%9B%BE%E7%89%8720181202130028.png" alt="Q图片2018120213002"></p>
<h2 id="Contributions-4"><a href="#Contributions-4" class="headerlink" title="Contributions"></a>Contributions</h2><ol>
<li>propose a new loss:<strong>mean-variance loss</strong>,aiming at the estimate of an age distribution with its mean as close to the groud-truth age as possible , and its variance as small as possible</li>
<li>diffferent from the age disturibution learning methods,the proposed approach <strong>doesn’t require that each training image must have a mean age and a variance</strong> (neither real nor assumed) labels during model training, but it can still give a distribution estimate for a face image</li>
<li>the proposed loss can be <strong>easily embedded into different CNNs</strong>, and the network can be optimized via SGD <strong>end-to-end</strong></li>
</ol>
<h2 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h2><p>思想简单且新颖，有用且有效。</p>
]]></content>
    
    <summary type="html">
    
      年龄估计的系列论文阅读
    
    </summary>
    
      <category term="论文笔记" scheme="calyp.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="年龄估计" scheme="calyp.github.io/tags/%E5%B9%B4%E9%BE%84%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>手册&amp;指南</title>
    <link href="calyp.github.io/2018/04/27/%E6%89%8B%E5%86%8C&amp;%E6%8C%87%E5%8D%97/"/>
    <id>calyp.github.io/2018/04/27/手册&amp;指南/</id>
    <published>2018-04-27T01:32:52.000Z</published>
    <updated>2018-12-05T09:25:16.952Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linux操作相关"><a href="#Linux操作相关" class="headerlink" title="Linux操作相关"></a>Linux操作相关</h1><p>##基本操作<br><strong>打包</strong>：<br>​       <code>tar -cvf xx.tar xxx</code><br>​       <code>tar zcvf xx.tar.gz xx</code><br><strong>解压</strong>：<br>       <code>tar zxvf xx.tar.gz</code><br>       <code>tar xvf xx.tar</code><br>       <code>tar jxvf xx.tar.bz2</code><br>       <code>bzip2 -d xx.bz2</code><br><strong>创建目录xx</strong>：<code>mkdir xx</code><br><strong>创建目录a及以下子目录b</strong>:<code>mkdir -p a/b</code><br><strong>创建xx文件</strong>:<code>touch xx</code><br><strong>查看xx文件</strong>:<code>cat xx</code><br><strong>查看xx文件</strong>:<code>less xx</code>(内容多于一屏时，j/k向下/上翻滚)<br><strong>显示绝对路径</strong>：<code>pwd</code><br><strong>删除空目录</strong>：<code>rmdir</code><br><strong>删除xx文件</strong>：<code>rm xx</code><br><strong>强制删除目录a</strong>:<code>rm -rf a</code><br><strong>删除目录a</strong>:<code>rm -r a</code><br><strong>删除父目录a中所有的空子目录</strong>：<code>cd a; rmdir</code><br><strong>复制文件a到文件b</strong>：<code>cp a b</code><br><strong>复制目录a为目录b</strong>：<code>cp -r a/ b/</code><br><strong>从指定文件搜索指定文件</strong>：<code>grep &#39;指定内容&#39; ./ -R</code><br><strong>下载文件到xx目录</strong>：<code>axel http://... -o xx</code><br><strong>给文件a.c加执行权限</strong>：<code>chmod +x a.c</code><br><strong>运行脚本xx.sh</strong>:<code>sh xx.sh</code><br><strong>使.bashrc生效</strong>：<code>source ~/.bashrc</code><br><strong>查看opencv版本</strong>：<code>pkg-config --modversion opencv</code><br><strong>查看opencv安装位置</strong>：<code>pkg-config -cflags opencv</code><br><strong>查找</strong>：<code>grep -n -H -R &quot;you want to search &quot; *</code><br><strong>查看文件夹大小</strong>：<code>du -sh</code><br><strong>查看文件夹大小并排序</strong>：<code>du -h /home/* | sort</code><br><strong>查看文件夹大小</strong>：<code>du -h --max-depth=1 ./</code><br><strong>查看磁盘的使用情况</strong>：<code>df -h</code><br><strong>查看权限</strong>：<code>ls -l filaname</code><br><strong>查看GPU使用率</strong>：<code>nvidia-smi</code><br><strong>查看CPU使用率</strong>：<code>top</code><br><strong>查看文件夹内文件个数</strong>：<code>ls -l | grep &#39;^-&#39; | wc -l</code><br><strong>查看文件夹内目录个数</strong>：<code>ls -l | grep &#39;^d&#39; | wc -l</code><br><strong>查看文件行数</strong>：<code>wc -l filename</code><br><strong>批量更改图片尺寸</strong>：<code>mogrify -resize 224x224 -format jpg *</code></p>
<p><strong>查看网络设置</strong>：<code>ifconfig</code><br><strong>查看操作系统</strong>：<code>uname -a</code><br><strong>查看ubuntu版本</strong>：<code>lsb_release -a</code><br><strong>查看操作系统位数</strong>：getconf LONG_BIT<br><strong>查看gcc版本</strong>：<code>ls /usr/bin/gcc*或gcc -v</code><br><strong>查看磁盘使用情况</strong>：<code>baobab</code><br><strong>添加用户</strong>：<code>sudo adduser username</code><br><strong>删除用户</strong>: <code>sudo userdel -r username</code><br><strong>添加sudo权限</strong>：<code>sudo vi /etc/sudoers</code></p>
<p><strong>显示终端上所有进程</strong>：<code>ps -a</code><br><strong>查看进程所有者及其他详细信息</strong>：<code>ps -u</code><br><strong>杀掉某进程</strong>：<code>kill -SIGKILL 进程号</code><br><strong>查看使用apt-get的进程</strong>：<code>ps -aux | grep &#39;apt-get&#39;</code><br><strong>进程暂停</strong>：<code>kill -STOP pid</code><br><strong>进程重启</strong>：<code>kill -CONT pid</code></p>
<p><strong>替换gcc版本</strong>：<br><code>sudo apt-get install gcc-4.9 gcc-4.9-multilib g++-4.9 g++-4.9-multilib</code><br><code>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 40</code><br><code>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5.4 50</code><br><code>sudo update-alternatives --config gcc</code><br><code>sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 50</code><br><code>sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 40</code><br><code>sudo update-alternatives --remove gcc /usr/bin/gcc-4.9</code></p>
<p><strong>查找命令：</strong></p>
<blockquote>
<ul>
<li>find . -name ‘my*’：搜索当前目录（含子目录，以下同）中，所有文件名以my开头的文</li>
<li>locate ~/m ：搜索用户主目录下，所有以m开头的文件</li>
<li>locate -i ~/m：搜索用户主目录下，所有以m开头的文件，并且忽略大小写</li>
<li>whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息</li>
<li>which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令</li>
</ul>
</blockquote>
<p><strong>Zip相关</strong>：</p>
<blockquote>
<ul>
<li>把一个文件abc.txt和一个目录dir1压缩成为yasuo.zip：zip -r yasuo.zip abc.txt dir1</li>
<li>下载了一个yasuo.zip文件，想解压缩：unzip yasuo.zip</li>
<li>当前目录下有abc1.zip，abc2.zip和abc3.zip，一起解压缩它们：unzip abc\?.zip(注释：?表示一个字符，如果用*表示任意多个字符。)</li>
<li>有一个很大的压缩文件large.zip，不想解压缩，只想看看它里面有什么：unzip -v large.zip</li>
<li>下载了一个压缩文件large.zip，想验证一下这个压缩文件是否下载完全了: unzip -t large.zip</li>
<li>用-v选项发现music.zip压缩文件里面有很多目录和子目录，并且子目录中其实都是歌曲mp3文件，把这些文件都下载到第一级目录，而不是一层一层建目录： unzip -j music.zip</li>
</ul>
</blockquote>
<h1 id="Vim相关"><a href="#Vim相关" class="headerlink" title="Vim相关"></a>Vim相关</h1><p><strong>跳转到指令行</strong>：在命令行模式下输入  “:行号”<br><strong>查找字符串</strong>：在命令行模式下输入 “/字符串”，按“n”键查找下一个<br><strong>批量替换</strong>：在命令行模式下输入 “:%s#abc#def#g”   将def替换abc<br><strong>批量注释</strong>：ctrl+v进入列模式，大写I进入插入模式，输入注释符//或#,连按两次esc。<br><strong>取消批量注释</strong>：Ctrl + v 进入块选择模式，选中你要删除的行首的注释符号，注意// 要选中两个，选好之后按d即可删除注释</p>
<h1 id="安装驱动"><a href="#安装驱动" class="headerlink" title="安装驱动"></a>安装驱动</h1><p><strong>查看显卡驱动信息</strong>：cat /proc/driver/nvidia/version<br><strong>查询合适的驱动版本xxx</strong>: <a href="https://www.geforce.com/drivers" target="_blank" rel="external">https://www.geforce.com/drivers</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">sudo apt-get remove --purge nvidia-*</div><div class="line">sudo add-apt-repository ppa:graphics-drivers/ppa</div><div class="line">sudo apt-get update</div><div class="line">sudo service lightdm stop</div><div class="line">sudo apt-get install nvidia-XXX</div><div class="line">sudo service lightdm start</div><div class="line">sudo reboot</div><div class="line">lsmod | grep nvidia   #查看驱动状态是否正常</div></pre></td></tr></table></figure></p>
<h1 id="Matlab相关"><a href="#Matlab相关" class="headerlink" title="Matlab相关"></a>Matlab相关</h1><p>后台运行：nohup matlab -nojvm -nodisplay -nosplash &lt; matlabscript.m 1&gt;running.log 2&gt;running.err &amp;</p>
<h1 id="Python相关"><a href="#Python相关" class="headerlink" title="Python相关"></a>Python相关</h1><p>##路径<br>import os<br>先定义一个带路径的文件<br>filename = “/home/mydir/test.txt”<br>将文件路径分割出来<br>file_dir = os.path.split(filename )[0]<br>判断文件路径是否存在，如果不存在，则创建，此处是创建多级目录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">if not os.path.isdir(file_dir):</div><div class="line">    os.makedirs(file_dir)</div></pre></td></tr></table></figure></p>
<p>然后再判断文件是否存在，如果不存在，则创建<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">if not os.path.exists(filename ):</div><div class="line">    os.system(r&apos;touch %s&apos; % filename)</div></pre></td></tr></table></figure></p>
<h1 id="git相关"><a href="#git相关" class="headerlink" title="git相关"></a>git相关</h1><p>设置Git的user name和email</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git config --global user.name &quot;yourname&quot;</div><div class="line">git config --global user.email &quot;youremail&quot;</div></pre></td></tr></table></figure>
<p>生成SSH密钥</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">查看是否已经有了ssh密钥：cd ~/.ssh</div><div class="line">如果没有密钥则不会有此文件夹，有则备份删除</div><div class="line">生存密钥：</div><div class="line">ssh-keygen -t rsa -C “haiyan.xu.vip@gmail.com”</div><div class="line">按3个回车，密码为空。</div><div class="line">Your identification has been saved in /home/tekkub/.ssh/id_rsa.</div><div class="line">Your public key has been saved in /home/tekkub/.ssh/id_rsa.pub.</div><div class="line">The key fingerprint is:</div><div class="line">………………</div><div class="line">最后得到了两个文件：id_rsa和id_rsa.pub</div></pre></td></tr></table></figure>
<p>添加密钥到ssh：ssh-add 文件名,需要之前输入密码.</p>
<p>在github上添加ssh密钥，这要添加的是“id_rsa.pub”里面的公钥。</p>
<p>从本地上传到github:<br>git init<br>git remote add origin <a href="https://github.com/nerddd/text.git" target="_blank" rel="external">https://github.com/nerddd/text.git</a>  </p>
]]></content>
    
    <summary type="html">
    
      关于linux/vim/matlab/python/git等的一些指令
    
    </summary>
    
      <category term="技能" scheme="calyp.github.io/categories/%E6%8A%80%E8%83%BD/"/>
    
    
      <category term="工具" scheme="calyp.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>人脸识别回顾</title>
    <link href="calyp.github.io/2018/01/29/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E5%9B%9E%E9%A1%BE/"/>
    <id>calyp.github.io/2018/01/29/人脸识别回顾/</id>
    <published>2018-01-29T01:47:23.000Z</published>
    <updated>2018-12-05T09:24:22.556Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h1><p><strong>数据清洗</strong>：去除错误标签，图片质量不好的样本，如果有benchmark，还需要去除和benchmark重复的样本。</p>
<p><strong>构造验证集</strong>:  对于给定的数据集，可能没有划分train set和validation set ，需要手动从给定的训练集中按照一定的比例分离出验证集（比如9:1）</p>
<p><strong>数据均衡</strong>：数据集可能存在类别不均衡的问题，可以通过重新组合不均衡数据, 使之均衡，方式一: 复制或者合成（比如jitter操作）少数部分的样本, 使之和多数部分差不多数量； 方式二: 砍掉一些多数部分, 使两者数量差不多</p>
<p><strong>数据扩充</strong>：对于一些样本数据比较少的数据集，为了更好的训练网络，有时候需要人为增加一些训练样本，比如随机的剪裁、缩放和旋转等。</p>
<p><strong>预处理</strong>：常见的就是减均值、除方差。</p>
<h1 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h1><h2 id="1-模型选择："><a href="#1-模型选择：" class="headerlink" title="1. 模型选择："></a>1. 模型选择：</h2><p>根据具体任务和数据集规模选择合适的网络结构，对于分类任务来说，如果数据集的规模不大，则网络的层数不应太深，结构也不应太复杂。</p>
<h2 id="2-激励函数的选择"><a href="#2-激励函数的选择" class="headerlink" title="2. 激励函数的选择"></a>2. 激励函数的选择</h2><ul>
<li><p><strong>sigmoid函数</strong>：取值范围为(0,1)，可以将一个实数映射到(0,1)的区间，可以用来做二分类，在特征相差比较复杂或是相差不是特别大时效果比较好，缺点是：激活函数计算量大，反向传播求梯度时，求导涉及除法，反向传播时，很容易出现梯度消失的情况</p>
</li>
<li><p><strong>Tanh函数</strong>：取值范围为[-1,1]，在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果，与sigmoid的区别是,tanh是0均值的，因此实际应用中，tanh会比sigmoid更好。</p>
</li>
<li><p><strong>ReLU函数</strong>：使用ReLU得到的SGD的收敛速度会比sigmoid/tanh快很多，缺点是训练的时候很”脆弱”，很容易就”die”了例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。</p>
<p>选择的时候，就是根据各个函数的优缺点来配置，例如：</p>
<p>如果使用 ReLU，要小心设置 learning rate，注意不要让网络出现很多 “dead” 神经元，如果不好解决，可以试试 Leaky ReLU、PReLU 或者 Maxout.</p>
<p><strong>详细细节请参看</strong>：<a href="https://www.jianshu.com/p/22d9720dbf1a" target="_blank" rel="external">常用激活函数比较</a></p>
</li>
</ul>
<h2 id="3-卷积tricks"><a href="#3-卷积tricks" class="headerlink" title="3. 卷积tricks"></a>3. 卷积tricks</h2><p>图片输入是2的幂次方，例如32、64、96、224等。</p>
<p>卷积核大小是3<em>3或者5</em>5。</p>
<p>输入图片上下左右需要用0补充，即padding，且假如卷积核大小是5，那么padding就是2（图片左右上下都补充2）；卷积核大小是3，padding大小就是1。</p>
<h2 id="4-pooling层tricks"><a href="#4-pooling层tricks" class="headerlink" title="4. pooling层tricks"></a>4. pooling层tricks</h2><p>poolin层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。max pooling比avg pooling效果会好一些。avg-global pooling进入全卷积时代。</p>
<h2 id="5-Loss函数的选择"><a href="#5-Loss函数的选择" class="headerlink" title="5. Loss函数的选择"></a>5. Loss函数的选择</h2><ul>
<li><p><strong>softmax loss</strong></p>
</li>
<li><p>contrastive loss</p>
</li>
<li><p>triplet loss</p>
</li>
<li><p><strong>center loss</strong></p>
<p><strong>triplet loss比softmax的优势</strong></p>
<ul>
<li>在于softmax不直接，（三元组直接优化距离），因而性能也不好。</li>
<li>softmax产生的特征表示向量都很大，一般超过1000维。</li>
<li>利用triplet loss的metric learning目的在于减小类内的L2距离，同时增大类间的距离</li>
</ul>
<p><strong>center loss VS triplet loss</strong></p>
<ul>
<li>triplet loss:dramatic data expansion</li>
<li>center loss:more directly and efficiently</li>
</ul>
</li>
</ul>
<h1 id="神经网络的设计模式"><a href="#神经网络的设计模式" class="headerlink" title="神经网络的设计模式"></a>神经网络的设计模式</h1><ol>
<li><p><strong>Architectural Structure follows the Application</strong>（架构遵循应用）</p>
<p>应该根据自己的应用场景选择合适的网络架构。</p>
</li>
<li><p><strong>Proliferate Paths</strong>（路径扩增）</p>
<p>每年ImageNet Challenge的赢家都比上一年的冠军使用更加深层的网络。从AlexNet 到Inception到Resnets，Smith和他的团队也观察到“网络的路径数量成倍增长”的趋势，ResNet可以是不同长度的网络的指数集合。</p>
</li>
<li><p><strong>Strive for Simplicity</strong>（简洁原则）</p>
<p>更大的并不一定是更好的。在名为“Bigger is not necessarily better”的论文中，Springenberg 等人演示了如何用更少的单元实现最先进的结果。</p>
</li>
<li><p><strong>Increase Symmetry</strong> （增加对称性）</p>
<p>无论是在建筑上，还是在生物上，对称性被认为是质量和工艺的标志。Smith 将 FractalNet 的优雅归功于网络的对称性。</p>
</li>
<li><p><strong>Pyramid Shape</strong> （金字塔型）</p>
<p>在表征能力和减少冗余或者无用信息之间权衡。CNNs通常会降低激活函数的采样，并会增加从输入层到最终层之间的连接通道。</p>
</li>
<li><p><strong>Over-train</strong> （过训练）</p>
<p>另一个权衡是训练准确率和泛化能力。用正则化的方法类似 drop-out 或 drop-path进行提升泛化能力，这是神经网络的重要优势。用比实际用例更难的问题训练你的网络，以提高泛化性能。</p>
</li>
<li><p><strong>Cover the Problem Space</strong> （覆盖问题空间）</p>
<p>为了扩大你的训练数据和提升泛化能力，要使用噪声和人工增加训练集的大小，例如随机旋转、裁剪和一些图像操作。</p>
</li>
<li><p><strong>Incremental Feature Construction</strong> （递增的功能结构）</p>
<p>当架构变得成功时，它们会简化每一层的“工作”。在非常深的神经网络中，每个 层只会递增地修改输入。在ResNets中，每一层的输出可能类似于输入。所以，在实践中，请在ResNet中使用短的跳过长度。</p>
</li>
<li><p><strong>Normalize Layer Inputs</strong> （规范化层输入）</p>
<p>标准化是可以使计算层的工作变得更加容易的一条捷径，并且在实际中可以提升训练的准确性。批量标准化的发明者认为标准化发挥作用的原因在于处理内部的协变量，但Smith 认为，“标准化把所有层的输入样本放在了一个平等的基础上（类似于单位转换），这允许反向传播可以更有效地训练”。</p>
</li>
<li><p><strong>Input Transition</strong> （输入转换）</p>
<p>研究表明，在Wide ResNets中,性能随着通道数量的增加而提升，但是要权衡训练成本与准确性。AlexNet，VGG，Inception和ResNets都是在第一层中进行输入变换，以保证多种方式检查输入数据。</p>
</li>
<li><p><strong>Available Resources Guide Layer Widths</strong> （可用资源决定层宽度）</p>
<p>可供选择的输出数量并不明显，相应的是，这取决于您的硬件功能和所需的准确性。</p>
</li>
<li><p><strong>Summation Joining</strong> （求和连接）</p>
<p>Summation是一种流行的合并分支的方式。在 ResNets 中，使用求和作为连接机制可以让每一个分支都能计算残差和整体近似。如果输入跳跃连接始终存在，那么Summation会让每一层学到正确地东西（例如：输入的差别）。在任何分支都可以被丢弃的网络（例如 FractalNet）中，你应该使用这种方式保持输出的平滑。</p>
</li>
<li><p><strong>Down-sampling Transition</strong> （下采样变换）</p>
<p>在汇聚的时候，利用级联连接来增加输出的数量。当使用大于1的步幅时，这会同时处理加入并增加通道的数量。</p>
</li>
<li><p><strong>Maxout for Competition </strong> （用于竞争的Maxout）</p>
<p>Maxout 用在只需要选择一个激活函数的局部竞争网络中。用的方法包含所有的激活函数，不同之处在于 maxout 只选择一个“胜出者”。Maxout 的一个明显的用例是每个分支具有不同大小的内核，而 Maxout 可以尺度不变。</p>
</li>
</ol>
<h1 id="Caffe调参"><a href="#Caffe调参" class="headerlink" title="Caffe调参"></a>Caffe调参</h1><p>##1. loss为nan(83.3365)  [标签错误、学习率太大]</p>
<p><strong>梯度爆炸</strong></p>
<p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续</p>
<p><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。</p>
<p><strong>措施</strong>： </p>
<ol>
<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>
<li>设置clip gradient，用于限制过大的diff</li>
</ol>
<p><strong>不当的损失函数</strong></p>
<p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p>
<p><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>
<p><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。</p>
<p>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>
<p><strong>不当的输入</strong></p>
<p><strong>原因</strong>：输入中就含有NaN。</p>
<p><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>
<p><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。</p>
<p><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>
<p><strong>池化层中步长比核的尺寸大</strong></p>
<p>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">  name: &quot;faulty_pooling&quot;</div><div class="line">  type: &quot;Pooling&quot;</div><div class="line">  bottom: &quot;x&quot;</div><div class="line">  top: &quot;y&quot;</div><div class="line">  pooling_param &#123;</div><div class="line">  pool: AVE</div><div class="line">  stride: 5</div><div class="line">  kernel: 3</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>##2. 避免overfitting</p>
<ul>
<li><p>simpler model structure</p>
</li>
<li><p>regularization</p>
</li>
<li><p>data augmentation</p>
</li>
<li><p>dropall(dropout+drop-path)</p>
</li>
<li><p>Bootstrap/Bagging</p>
</li>
<li><p>ensemble</p>
</li>
<li><p>early stopping</p>
</li>
<li><p>utilize invariance</p>
</li>
<li><p>Bayesian</p>
<p>​</p>
</li>
</ul>
<h1 id="人脸识别流程"><a href="#人脸识别流程" class="headerlink" title="人脸识别流程"></a>人脸识别流程</h1><p><strong>一般流程</strong>:数据准备-&gt;人脸检测-&gt;人脸对齐-&gt;生成数据文件-&gt;训练模型-&gt;调参-&gt;model</p>
<p><strong>tricks</strong>:</p>
<ul>
<li>在数据准备阶段，可以采用jitter等操作对样本数量较少的种类进行扩充，弥补类别不均衡造成的影响</li>
<li>在生成模型阶段，可以采用ensemble的方式对多个模型进行融合，比如mirror。</li>
</ul>
<p><strong>参考及致谢</strong></p>
<p><a href="https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training" target="_blank" rel="external">Common causes of nans during training</a></p>
<p><a href="https://www.jianshu.com/p/22d9720dbf1a" target="_blank" rel="external">常用激活函数比较</a></p>
<p><a href="https://chenrudan.github.io/blog/2015/08/04/dl5tricks.html" target="_blank" rel="external">深度学习之五常见tricks</a></p>
<p>《Deep convolutional neural network design patterns 》</p>
]]></content>
    
    <summary type="html">
    
      人脸识别的一般流程
    
    </summary>
    
      <category term="深度学习" scheme="calyp.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="人脸识别" scheme="calyp.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>DeepID3论文笔记</title>
    <link href="calyp.github.io/2017/07/04/DeepID3%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>calyp.github.io/2017/07/04/DeepID3论文笔记/</id>
    <published>2017-07-04T02:34:31.000Z</published>
    <updated>2018-12-05T09:20:35.318Z</updated>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>深层的网络由于良好的学习能力，近年来在目标识别领域取得巨大成功，基于此，提出两个非常深的网络架构，DeepID3，这两个架构分别基于VGG net和GoogLeNet通过改进使之用于人脸识别。Joint face identification-verification supervisory signals are added to both intermediate and<br>final feature extraction layers during training.        </p>
]]></content>
    
    <summary type="html">
    
      DeepID3论文阅读笔记
    
    </summary>
    
      <category term="论文笔记" scheme="calyp.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Face" scheme="calyp.github.io/tags/Face/"/>
    
      <category term="笔记" scheme="calyp.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Center loss笔记</title>
    <link href="calyp.github.io/2017/06/29/Center-loss%E7%AC%94%E8%AE%B0/"/>
    <id>calyp.github.io/2017/06/29/Center-loss笔记/</id>
    <published>2017-06-29T09:05:51.000Z</published>
    <updated>2018-12-05T09:29:14.096Z</updated>
    
    <content type="html"><![CDATA[<p>论文：<a href="https://link.springer.com/chapter/10.1007%2F978-3-319-46478-7_31" target="_blank" rel="external">A Discriminative Feature Learning Approach for Deep Face Recognition</a></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>对于一般的CNN网络，softmax通常作为监督信号来训练深层网络，为了增强提取的特征的可辨别性（discriminative），提出center loss，应对人脸识别任务。center loss同时学习每个类别的深层特征中心和惩罚深层特征和它们对应的类中心的距离（the center loss simultaneously learns a center for deep<br>features of each class and penalizes the distances between the deep features and their corresponding class centers）。将softmax和center loss联合起来，可以训练一个健壮的CNN来获取深层特征的两个关键目标，类内紧凑和类间分散。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>预先收集所有可能的测试身份用于训练是不现实的，所以CNN的标签预测并不总是适用的。经过深层网络获取得到的特征不仅需要可分开性(separable)，更需要识别性(discriminative)和广义性，足够用来识别新的没有遇见过的类。可识别性的特征可以利用最邻近(NN)或k-NN算法很好的分类，就没有必要依赖标签预测了。但是softmax只能产生可分开(separable)特征，结果特征就不足以用以人脸识别。</p>
<p><img src="https://static.leiphone.com/uploads/new/article/740_740/201612/585bb8742235c.png?imageMogr2/format/jpg/quality/90" alt="Separabale Feature Vs Discriminative Feature"></p>
<p>因为随机梯度下降(SGD)优化CNN是基于mini-batch，不能很好的反映深层特征的全局分布，由于训练集的庞大，在每次迭代中输入所有的训练样本是不现实的。constractive loss和triplet loss分别作为图像对和三元组的loss函数。然而，与图像样本相比，训练图像对或三元组的数量显著增长，导致收敛缓慢和不稳定性。仔细选择图像对或者三元组，问题可能会部分缓解，但是它增加了计算复杂度，训练过程变得不方便。为了解决这个问题，提出center loss，用于有效的增强特征的可识别性，我们将会得到每个类的深层特征的中心。在训练阶段，我们同时更新中心和最小化特征和它们相应的类中心的距离。CNN同时在softmax loss和center loss的监督下进行训练，通过一个超参来平衡这两个监督信号。直觉上，softmax loss将不同类别的特征分开，center loss有效的将同一类别的特征拉向类的中心，使得类内特征分布变得紧凑。</p>
]]></content>
    
    <summary type="html">
    
      Center loss论文阅读笔记
    
    </summary>
    
      <category term="论文笔记" scheme="calyp.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习，论文笔记" scheme="calyp.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%8C%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>FaceNet论文笔记</title>
    <link href="calyp.github.io/2017/06/28/FaceNet%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    <id>calyp.github.io/2017/06/28/FaceNet论文笔记/</id>
    <published>2017-06-28T02:11:42.000Z</published>
    <updated>2018-12-05T09:29:09.723Z</updated>
    
    <content type="html"><![CDATA[<p><em>原文链接</em>:<a href="https://arxiv.org/abs/1503.03832" target="_blank" rel="external">FaceNet:A unified embedding for face recognition and clustering</a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>FaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。</p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>FaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。</p>
<p>当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。选取的triplets包含两个匹配脸部缩略图（为紧密裁剪的脸部区域，不用使用2d、3d对齐以及放大转换等预处理）和一个非匹配的脸部缩略图，loss函数目标是通过距离边界区分正负类。</p>
<p>本文中，探索了两类深度卷积神经网络，第一类为Zeiler&amp;Fergus研究中使用的神经网络，包含多个交错的卷积层、非线性激励函数，局部相应归一化和最大池化层。我们额外的添加了一些1x1xd的卷积层。第二种结构是基于Inception model，这种网络利用了一些不同的卷积层和池化层并行和级联响应。我们发现这些模型可以减小20倍以上的参数数量，并且可能会减少FLOPS数量。</p>
<p>triplet loss的启发是传统loss函数趋向于将有一类特征的人脸图像映射到同一个空间，而triplet loss尝试将一个个体的人脸图像和其他人脸图像分开。</p>
<p><img src="/images/facenet_triplet1.png" alt="facenet_triplet1"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>三元组的目标函数并不是这篇论文首创，我在之前的一些Hash索引的论文中也见过相似的应用。可见，并不是所有的学习特征的模型都必须用softmax。用其他的效果也会好。</li>
<li>三元组比softmax的优势在于<ul>
<li>softmax不直接，（三元组直接优化距离），因而性能也不好。</li>
<li>softmax产生的特征表示向量都很大，一般超过1000维。</li>
</ul>
</li>
<li>FaceNet并没有像DeepFace和DeepID那样需要对齐。</li>
<li>FaceNet得到最终表示后不用像DeepID那样需要再训练模型进行分类，直接计算距离就好了，简单而有效。</li>
<li>论文并未探讨二元对的有效性，直接使用的三元对。</li>
</ul>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://zhuanlan.zhihu.com/p/24837264" target="_blank" rel="external">谷歌人脸识别系统FaceNet解析</a></p>
<p><a href="http://blog.csdn.net/stdcoutzyx/article/details/46687471" target="_blank" rel="external">FaceNet–Google的人脸识别</a></p>
]]></content>
    
    <summary type="html">
    
      FaceNet论文阅读笔记
    
    </summary>
    
      <category term="论文笔记" scheme="calyp.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="笔记，人脸识别" scheme="calyp.github.io/tags/%E7%AC%94%E8%AE%B0%EF%BC%8C%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>Caffe调参</title>
    <link href="calyp.github.io/2017/06/17/Caffe%E8%B0%83%E5%8F%82/"/>
    <id>calyp.github.io/2017/06/17/Caffe调参/</id>
    <published>2017-06-17T01:26:12.000Z</published>
    <updated>2019-03-01T03:10:23.884Z</updated>
    
    <content type="html"><![CDATA[<h1 id="loss为nan"><a href="#loss为nan" class="headerlink" title="loss为nan"></a>loss为nan</h1><h2 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h2><p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续<br><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。<br><strong>措施</strong>： </p>
<ol>
<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>
<li>设置clip gradient，用于限制过大的diff</li>
</ol>
<h2 id="不当的损失函数"><a href="#不当的损失函数" class="headerlink" title="不当的损失函数"></a>不当的损失函数</h2><p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。<br><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。<br><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。<br>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>
<h2 id="不当的输入"><a href="#不当的输入" class="headerlink" title="不当的输入"></a>不当的输入</h2><p><strong>原因</strong>：输入中就含有NaN。<br><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。<br><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。<br><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>
<p><strong>池化层中步长比核的尺寸大</strong><br>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">  name: &quot;faulty_pooling&quot;</div><div class="line">  type: &quot;Pooling&quot;</div><div class="line">  bottom: &quot;x&quot;</div><div class="line">  top: &quot;y&quot;</div><div class="line">  pooling_param &#123;</div><div class="line">  pool: AVE</div><div class="line">  stride: 5</div><div class="line">  kernel: 3</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><strong>致谢</strong><br><em><a href="http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training" target="_blank" rel="external">http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training</a></em></p>
<h1 id="Accuracy一直为0"><a href="#Accuracy一直为0" class="headerlink" title="Accuracy一直为0"></a>Accuracy一直为0</h1><p>考虑标签是否从0开始递增</p>
]]></content>
    
    <summary type="html">
    
      caffe调参的一些经验
    
    </summary>
    
      <category term="深度学习" scheme="calyp.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="caffe" scheme="calyp.github.io/tags/caffe/"/>
    
      <category term="调参" scheme="calyp.github.io/tags/%E8%B0%83%E5%8F%82/"/>
    
  </entry>
  
  <entry>
    <title>Python</title>
    <link href="calyp.github.io/2017/06/09/Python/"/>
    <id>calyp.github.io/2017/06/09/Python/</id>
    <published>2017-06-09T12:52:56.000Z</published>
    <updated>2018-12-05T09:21:41.670Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像操作"><a href="#图像操作" class="headerlink" title="图像操作"></a>图像操作</h1><h2 id="关于PIL-image和skimage的图像处理"><a href="#关于PIL-image和skimage的图像处理" class="headerlink" title="关于PIL image和skimage的图像处理"></a>关于PIL image和skimage的图像处理</h2><h3 id="对skimage图像"><a href="#对skimage图像" class="headerlink" title="对skimage图像"></a>对skimage图像</h3><p>镜像处理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">from skimage import io,transform</div><div class="line">import matplotlib.pyplot as plt</div><div class="line">import cv2</div><div class="line"></div><div class="line">img=io.imread(&quot;test.jpg&quot;)</div><div class="line">#img=transform.rotate(img,180)</div><div class="line">img=cv2.flip(img,1)</div><div class="line">plt.figure(&apos;skimage&apos;)</div><div class="line">plt.imshow(img)</div><div class="line">plt.show()</div><div class="line">print img.shape</div><div class="line">print(img.dtype)</div></pre></td></tr></table></figure>
<h1 id="文本处理"><a href="#文本处理" class="headerlink" title="文本处理"></a>文本处理</h1><h2 id="提取两个文件中相同的部分"><a href="#提取两个文件中相同的部分" class="headerlink" title="提取两个文件中相同的部分"></a>提取两个文件中相同的部分</h2><p>a.txt内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">aaa 0</div><div class="line">bbb 0</div><div class="line">ccc 0</div></pre></td></tr></table></figure>
<p>b.txt内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">aaa 0</div><div class="line">bbb 1</div><div class="line">ccc 0</div></pre></td></tr></table></figure>
<h3 id="提取相同的部分"><a href="#提取相同的部分" class="headerlink" title="提取相同的部分"></a>提取相同的部分</h3><p>写入到c.txt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">fa=open(&apos;a.txt&apos;,&apos;r&apos;)</div><div class="line">a=fa.readlines()</div><div class="line">fa.close()</div><div class="line">fb=open(&apos;b.txt&apos;,&apos;r&apos;)</div><div class="line">b=fb.readlines()</div><div class="line">fb.close()</div><div class="line">c= [i for i in a if i in b]</div><div class="line">fc=open(&apos;c.txt&apos;,&apos;w&apos;)</div><div class="line">fc.writelines(c)</div><div class="line">fc.close()</div><div class="line">print &apos;Done&apos;</div></pre></td></tr></table></figure>
<p>最后c.txt内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">aaa 0</div><div class="line">ccc 0</div></pre></td></tr></table></figure>
<h1 id="读取文件并绘制图片"><a href="#读取文件并绘制图片" class="headerlink" title="读取文件并绘制图片"></a>读取文件并绘制图片</h1><h2 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">import matplotlib</div><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line">def loadData(fileName):</div><div class="line">    inFile=open(fileName,&apos;r&apos;)</div><div class="line">    X = []</div><div class="line">    y = []</div><div class="line">    for line in inFile:</div><div class="line">        trainingSet = line.split()  </div><div class="line">        X.append(trainingSet[0])</div><div class="line">        y.append(trainingSet[1])</div><div class="line">    return (X, y)</div><div class="line"></div><div class="line"></div><div class="line">def plotData(X, y):</div><div class="line">    length = len(y)</div><div class="line">    plt.figure(1)</div><div class="line">    #plt.plot(X, y, &apos;rx&apos;)</div><div class="line">    plt.scatter(X,y,c=&apos;r&apos;,marker=&apos;.&apos;)</div><div class="line">    plt.xlabel(&apos;eye_width&apos;)</div><div class="line">    plt.ylabel(&apos;eye_height&apos;)</div><div class="line">    #plt.show()</div><div class="line">    plt.savefig(&apos;dis.png&apos;)</div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    (X, y) = loadData(&apos;dis.txt&apos;)</div><div class="line"></div><div class="line">    plotData(X, y)</div></pre></td></tr></table></figure>
<h2 id="折线图"><a href="#折线图" class="headerlink" title="折线图"></a>折线图</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt </div><div class="line"></div><div class="line">y1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] </div><div class="line">x1=range(0,200,10) </div><div class="line"></div><div class="line">num = 0</div><div class="line">for i in range(20):</div><div class="line">    num+=y1[i]</div><div class="line">print num</div><div class="line">plt.plot(x1,y1,label=&apos;Frist line&apos;,linewidth=1,color=&apos;r&apos;,marker=&apos;o&apos;, </div><div class="line">markerfacecolor=&apos;blue&apos;,markersize=6) </div><div class="line">plt.xlabel(&apos;eye_width distribute&apos;) </div><div class="line">plt.ylabel(&apos;Num&apos;) </div><div class="line">plt.title(&apos;Eye\nCheck it out&apos;) </div><div class="line">plt.legend()</div><div class="line">plt.savefig(&apos;figure.png&apos;) </div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<h1 id="统计文件中的数据分布"><a href="#统计文件中的数据分布" class="headerlink" title="统计文件中的数据分布"></a>统计文件中的数据分布</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">import matplotlib</div><div class="line">import matplotlib.pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">def loadData(fileName):</div><div class="line">    inFile=open(fileName,&apos;r&apos;)</div><div class="line">    x_lines=inFile.readlines()#x_lines为str的list</div><div class="line">    x_distribute=[0]*20 #对列表元素进行重复复制</div><div class="line">    for x_line in x_lines:</div><div class="line">        x_point=x_line.split()[0]</div><div class="line">        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型</div><div class="line">        x_distribute[i]+=1</div><div class="line">    print x_distribute</div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    loadData(&apos;dis.txt&apos;)</div></pre></td></tr></table></figure>
<h1 id="windows下安装OpenCV-for-Python"><a href="#windows下安装OpenCV-for-Python" class="headerlink" title="windows下安装OpenCV for Python"></a>windows下安装OpenCV for Python</h1><ol>
<li><p>Download Python, Numpy, OpenCV from their official sites.</p>
</li>
<li><p>Extract OpenCV (will be extracted to a folder opencv)</p>
</li>
<li><p>Copy ..\opencv\build\python\x86\2.7\cv2.pyd</p>
</li>
<li><p>Paste it in C:\Python27\Lib\site-packages</p>
</li>
<li><p>Open Python IDLE or terminal, and type</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import cv2</div></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="读取文件，进行批量创建目录"><a href="#读取文件，进行批量创建目录" class="headerlink" title="读取文件，进行批量创建目录"></a>读取文件，进行批量创建目录</h1><p>存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">xxx/0.jpg</div><div class="line">yyy/0.jpg</div><div class="line">zzz/0.jpg</div></pre></td></tr></table></figure>
<p>创建xxx目录，yyy目录，zzz目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">import sys</div><div class="line">import os</div><div class="line">import numpy as np</div><div class="line">import skimage</div><div class="line">from skimage import io,transform</div><div class="line">import matplotlib.pyplot as plt</div><div class="line">import cv2</div><div class="line"></div><div class="line">def read_file(path):</div><div class="line">    with open(path) as f:</div><div class="line">        return list(f)</div><div class="line"></div><div class="line"></div><div class="line">def make_dir(image_path):   </div><div class="line">    image_lines = read_file(image_path)</div><div class="line">    if not image_lines:</div><div class="line">        print &apos;empty file&apos;</div><div class="line">        return</div><div class="line">    i = 0</div><div class="line">    for image_line in image_lines:</div><div class="line">        image_line = image_line.strip(&apos;\n&apos;)</div><div class="line">        subdir_name = image_line.split(&apos;/&apos;)[0]</div><div class="line">        print subdir_name</div><div class="line">        isExists=os.path.exists(subdir_name)</div><div class="line">        if not isExists:</div><div class="line">            os.mkdir(subdir_name)</div><div class="line">            print subdir_name+&quot;created successfully!&quot;</div><div class="line">       </div><div class="line">        i = i+1</div><div class="line">        sys.stdout.write(&apos;\riter %d\n&apos; %(i))</div><div class="line">        sys.stdout.flush()</div><div class="line"></div><div class="line">    print &apos;Done&apos;</div><div class="line"></div><div class="line">if __name__==&apos;__main__&apos;: </div><div class="line">    image_path=&apos;./image.txt&apos;</div><div class="line">    make_dir(image_path)</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      运用python处理一些简单问题的记录
    
    </summary>
    
    
      <category term="Python" scheme="calyp.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Triplet loss</title>
    <link href="calyp.github.io/2017/06/02/Triplet%20loss/"/>
    <id>calyp.github.io/2017/06/02/Triplet loss/</id>
    <published>2017-06-02T12:26:35.000Z</published>
    <updated>2018-12-05T09:29:18.170Z</updated>
    
    <content type="html"><![CDATA[<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>Triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。</p>
<p><img src="http://img.blog.csdn.net/20160727090101355" alt="Triplet Loss 示意图"></p>
<h3 id="Triplet-loss中的margin取值分析"><a href="#Triplet-loss中的margin取值分析" class="headerlink" title="Triplet loss中的margin取值分析"></a>Triplet loss中的margin取值分析</h3><p>我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。</div><div class="line"></div><div class="line">当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。</div><div class="line"></div><div class="line">因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。</div></pre></td></tr></table></figure>
<h2 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h2><p>区分相似图形，除了triplet loss，还有一篇CVPR：<a href="http://blog.csdn.net/u010167269/article/details/51783446" target="_blank" rel="external">《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》</a>提出的Coupled Cluster Loss.</p>
<p><strong>本文参考</strong>：</p>
<p><a href="http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html" target="_blank" rel="external">triplet loss 原理以及梯度推导</a></p>
<p><a href="http://www.voidcn.com/blog/mao_kun/article/p-6246924.html" target="_blank" rel="external">如何在Caffe中增加layer以及Caffe中triplet loss layer的实现</a></p>
]]></content>
    
    <summary type="html">
    
      Triplet loss论文阅读笔记
    
    </summary>
    
      <category term="论文笔记" scheme="calyp.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习，人脸识别" scheme="calyp.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%8C%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>Hexo相关</title>
    <link href="calyp.github.io/2017/06/02/Hexo%E7%9B%B8%E5%85%B3/"/>
    <id>calyp.github.io/2017/06/02/Hexo相关/</id>
    <published>2017-06-02T08:05:45.000Z</published>
    <updated>2018-12-05T09:21:07.498Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="Hexo无法正常显示公式"><a href="#Hexo无法正常显示公式" class="headerlink" title="Hexo无法正常显示公式"></a>Hexo无法正常显示公式</h2><p>善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># MathJax Support</div><div class="line">mathjax:</div><div class="line">  enable: true</div><div class="line">  per_page: true</div></pre></td></tr></table></figure>
<p>另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">title: index.html</div><div class="line">date: 2016-12-28 21:01:30</div><div class="line">tags:</div><div class="line">mathjax: true</div><div class="line">--</div></pre></td></tr></table></figure>
<p>题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">title: &#123;&#123; title &#125;&#125;</div><div class="line">date: &#123;&#123; date &#125;&#125;</div><div class="line">categories: </div><div class="line">tags:</div><div class="line">description: </div><div class="line">mathjax:</div></pre></td></tr></table></figure>
<h1 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h1><p><a href="https://formulahendry.github.io/2016/12/04/hexo-ci/" target="_blank" rel="external">Hexo的版本控制与持续集成</a></p>
]]></content>
    
    <summary type="html">
    
      Hexo相关的配置说明
    
    </summary>
    
      <category term="Hexo" scheme="calyp.github.io/categories/Hexo/"/>
    
    
      <category term="Hexo" scheme="calyp.github.io/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>待办及进度</title>
    <link href="calyp.github.io/2017/06/02/%E5%BE%85%E5%8A%9E%E5%8F%8A%E8%BF%9B%E5%BA%A6/"/>
    <id>calyp.github.io/2017/06/02/待办及进度/</id>
    <published>2017-06-02T07:15:46.000Z</published>
    <updated>2018-12-05T09:23:48.239Z</updated>
    
    <content type="html"><![CDATA[<h1 id="step1-Mirror-face相关"><a href="#step1-Mirror-face相关" class="headerlink" title="step1:Mirror face相关"></a>step1:Mirror face相关</h1><table>
<thead>
<tr>
<th>Model</th>
<th>PCA Size</th>
<th>Threshold</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mirror</td>
<td>192</td>
<td>0.64</td>
<td>99.42%</td>
</tr>
<tr>
<td>Mirror Concat</td>
<td>192</td>
<td>0.65</td>
<td>99.42%</td>
</tr>
<tr>
<td>Mirror Add/Average</td>
<td>184</td>
<td>0.64</td>
<td>99.47%</td>
</tr>
<tr>
<td>Mirror Max</td>
<td>144</td>
<td>0.65</td>
<td>99.43%</td>
</tr>
<tr>
<td>Mirror Min</td>
<td>168</td>
<td>0.65</td>
<td>99.48%</td>
</tr>
<tr>
<td>Mirror Avg+min</td>
<td>168</td>
<td>0.65</td>
<td>99.45%</td>
</tr>
</tbody>
</table>
<h1 id="step2-单眼Patch-model"><a href="#step2-单眼Patch-model" class="headerlink" title="step2:单眼Patch model"></a>step2:单眼Patch model</h1><table>
<thead>
<tr>
<th>eye_width</th>
<th>num</th>
<th>eye_width</th>
<th>num</th>
</tr>
</thead>
<tbody>
<tr>
<td>0~10</td>
<td>14</td>
<td>90~100</td>
<td>8,5674</td>
</tr>
<tr>
<td>10~20</td>
<td>3329</td>
<td>100~110</td>
<td>2,9481</td>
</tr>
<tr>
<td>20~30</td>
<td>21,3675</td>
<td>110~120</td>
<td>9051</td>
</tr>
<tr>
<td>30~40</td>
<td>45,1416</td>
<td>120~130</td>
<td>2894</td>
</tr>
<tr>
<td>40~50</td>
<td>49,1913</td>
<td>130~140</td>
<td>932</td>
</tr>
<tr>
<td>50~60</td>
<td>72,8911</td>
<td>140~150</td>
<td>279</td>
</tr>
<tr>
<td>60~70</td>
<td>137,9232</td>
<td>150~160</td>
<td>86</td>
</tr>
<tr>
<td>70~80</td>
<td>128,7442</td>
<td>160~170</td>
<td>14</td>
</tr>
<tr>
<td>80~90</td>
<td>30,9026</td>
<td>170~180</td>
<td>6</td>
</tr>
</tbody>
</table>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y…],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)<em>0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5</em>eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">if eye_width*0.5&gt;eye_left_x:</div><div class="line">            eye_width=eye_left_x*2</div></pre></td></tr></table></figure></p>
<p>导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张</p>
<p>重新生成眼睛宽度估算文件，其分布如下</p>
<table>
<thead>
<tr>
<th>eye_width</th>
<th>num</th>
<th>eye_width</th>
<th>num</th>
</tr>
</thead>
<tbody>
<tr>
<td>0~10</td>
<td>1,8975</td>
<td>100~110</td>
<td>98</td>
</tr>
<tr>
<td>10~20</td>
<td>71,3760</td>
<td>110~120</td>
<td>17</td>
</tr>
<tr>
<td>20~30</td>
<td>126,4102</td>
<td>120~130</td>
<td>3</td>
</tr>
<tr>
<td>30~40</td>
<td>226,4348</td>
<td>130~140</td>
<td>0</td>
</tr>
<tr>
<td>40~50</td>
<td>59,3351</td>
<td>140~150</td>
<td>0</td>
</tr>
<tr>
<td>50~60</td>
<td>10,6502</td>
<td>150~160</td>
<td>0</td>
</tr>
<tr>
<td>60~70</td>
<td>2,4588</td>
<td>160~170</td>
<td>1</td>
</tr>
<tr>
<td>70~80</td>
<td>5592</td>
<td>170~180</td>
<td>0</td>
</tr>
<tr>
<td>80~90</td>
<td>1588</td>
<td>180~190</td>
<td>8</td>
</tr>
<tr>
<td>90~100</td>
<td>421</td>
<td>190~200</td>
<td>21</td>
</tr>
</tbody>
</table>
<p>2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可<br>3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。<br>最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526</p>
<p><u>17个小时，迭代12万次，26.5%的准确率，loss=5.5。</u></p>
<h1 id="step3-双眼patch-model"><a href="#step3-双眼patch-model" class="headerlink" title="step3:双眼patch model"></a>step3:双眼patch model</h1><p>生成双眼宽度估算文件，其分布如下</p>
<table>
<thead>
<tr>
<th>eye_width</th>
<th>num</th>
<th>eye_width</th>
<th>num</th>
<th>eye_width</th>
<th>num</th>
</tr>
</thead>
<tbody>
<tr>
<td>0~10</td>
<td>30</td>
<td>100~110</td>
<td>18,8771</td>
<td>200~210</td>
<td>383</td>
</tr>
<tr>
<td>10~20</td>
<td>5888</td>
<td>110~120</td>
<td>8,8508</td>
<td>210~220</td>
<td>196</td>
</tr>
<tr>
<td>20~30</td>
<td>11,4595</td>
<td>120~130</td>
<td>4,6243</td>
<td>220~230</td>
<td>100</td>
</tr>
<tr>
<td>30~40</td>
<td>35,5451</td>
<td>130~140</td>
<td>2,4982</td>
<td>230~240</td>
<td>52</td>
</tr>
<tr>
<td>40~50</td>
<td>42,6729</td>
<td>140~150</td>
<td>1,2749</td>
<td>240~250</td>
<td>33</td>
</tr>
<tr>
<td>50~60</td>
<td>49,1312</td>
<td>150~160</td>
<td>6714</td>
<td>250~260</td>
<td>15</td>
</tr>
<tr>
<td>60~70</td>
<td>69,0394</td>
<td>160~170</td>
<td>3413</td>
<td>260~270</td>
<td>3</td>
</tr>
<tr>
<td>70~80</td>
<td>104,9353</td>
<td>170~180</td>
<td>1909</td>
<td>270~280</td>
<td>2</td>
</tr>
<tr>
<td>80~90</td>
<td>100,6618</td>
<td>180~190</td>
<td>1091</td>
<td>280~290</td>
<td>0</td>
</tr>
<tr>
<td>90~100</td>
<td>47,7219</td>
<td>190~200</td>
<td>622</td>
<td>290~300</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>筛选crop后，宽度在20~130区间的图片，共90523个类别，4932655张。</p>
<p>train.txt：3982004 </p>
<p>val.txt：950661 </p>
<p><u>18万次迭代之后，准确率只有66%左右。</u></p>
<h1 id="step4-crop对齐后的图片的眼睛，训练单眼模型"><a href="#step4-crop对齐后的图片的眼睛，训练单眼模型" class="headerlink" title="step4:crop对齐后的图片的眼睛，训练单眼模型"></a>step4:crop对齐后的图片的眼睛，训练单眼模型</h1><p>数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”）</p>
<pre><code>&quot;ref_points&quot;: [
        30.2946, 51.6963, 
     65.5318, 51.5014, 
     48.0252, 71.7366,
     33.5493, 92.3655, 
     62.7299, 92.2041
 ]


  eye_width=(ref_points[2]-ref_points[0])*0.8
     eye_height=eye_width
     x1=ref_points[0]-0.5*eye_width=16
     x2=ref_points[0]+0.5*eye_width=44
     y1=ref_points[1]-0.5*eye_height=37
     y2=ref_points[1]+0.5*eye_height=65
</code></pre><p>train:4071324 张</p>
<p>val:973183张</p>
<h1 id="step5-crop对齐后的图片的眼睛，训练双眼模型"><a href="#step5-crop对齐后的图片的眼睛，训练双眼模型" class="headerlink" title="step5:crop对齐后的图片的眼睛，训练双眼模型"></a>step5:crop对齐后的图片的眼睛，训练双眼模型</h1><p>数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”）</p>
<pre><code>   &quot;ref_points&quot;: [
       30.2946, 51.6963, 
    65.5318, 51.5014, 
    48.0252, 71.7366,
    33.5493, 92.3655, 
    62.7299, 92.2041
]
     eye_width=(ref_points[2]-ref_points[0])*0.8
    eye_height=eye_width
    x1=ref_points[0]-0.5*eye_width=16
    x2=ref_points[2]+0.5*eye_width=79
    y1=ref_points[1]-0.5*eye_height=37
    y2=ref_points[1]+0.5*eye_height=65
</code></pre><p><u>迭代16万次，精度为72.73%，loss=2.52</u></p>
<p><u>在lfw上测试，精度最高达到77.04%</u></p>
<h1 id="step6-Center-face-dropout-finetune-on-softmax"><a href="#step6-Center-face-dropout-finetune-on-softmax" class="headerlink" title="step6:Center face+dropout+finetune on softmax"></a>step6:Center face+dropout+finetune on softmax</h1><p>在msclean测试集上达到93.53% </p>
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th>PCA_Size</th>
<th>Threshold</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">dropcenter</td>
<td>168</td>
<td>0.64</td>
<td>99.42%</td>
</tr>
<tr>
<td style="text-align:left">dropcenter_mirror</td>
<td>136</td>
<td>0.64</td>
<td>99.38%</td>
</tr>
<tr>
<td style="text-align:left">dropcenter +dropcenter_mirror+Min</td>
<td>128</td>
<td>0.64</td>
<td>99.45%</td>
</tr>
<tr>
<td style="text-align:left">dropcenter+dropcenter_mirror+ Add</td>
<td>128</td>
<td>0.64</td>
<td>99.45%</td>
</tr>
<tr>
<td style="text-align:left">center +dropcenter +Min</td>
<td>400</td>
<td>0.64</td>
<td>99.40%</td>
</tr>
<tr>
<td style="text-align:left">centermirror+dropcenter+Min</td>
<td>128</td>
<td>0.64</td>
<td>99.45%</td>
</tr>
<tr>
<td style="text-align:left">centermirror+dropcenter+Add</td>
<td>160</td>
<td>0.64</td>
<td>99.43%</td>
</tr>
<tr>
<td style="text-align:left">centermirror+dropcenter+Max</td>
<td>160</td>
<td>0.64</td>
<td>99.43%</td>
</tr>
<tr>
<td style="text-align:left">centermirror+dropcenter+Concate</td>
<td>192</td>
<td>0.65</td>
<td>99.47%</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Model</th>
<th>PCA Size</th>
<th>Threshold</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>center_min_mirror+dropcenter+Concate</td>
<td>192</td>
<td>0.65</td>
<td>99.47%</td>
</tr>
<tr>
<td>center_min_mirror+dropcenter+Min</td>
<td>128</td>
<td>0.65</td>
<td>99.42%</td>
</tr>
<tr>
<td>center_min_mirror+dropcenter+Add</td>
<td>136</td>
<td>0.64</td>
<td>99.47%</td>
</tr>
<tr>
<td>eye_model</td>
<td>160</td>
<td>0.57</td>
<td>77.04%</td>
</tr>
<tr>
<td>eyemodel+center+Con</td>
<td>208</td>
<td>0.66</td>
<td>74.80%</td>
</tr>
<tr>
<td>三模型</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>center+center_min_mirror+dropoutcenter+Concate</td>
<td>128</td>
<td>0.65</td>
<td>99.43%</td>
</tr>
<tr>
<td>center+softmax+dropoutcenter+Concate</td>
<td>168</td>
<td>0.66</td>
<td>99.43%</td>
</tr>
<tr>
<td>center+softmax+dropoutcenter+Add</td>
<td>496</td>
<td>0.65</td>
<td>99.42%</td>
</tr>
</tbody>
</table>
<h1 id="step7-balance"><a href="#step7-balance" class="headerlink" title="step7:balance"></a>step7:balance</h1><h2 id="step7-1-减小过采样的数量，防止过拟合"><a href="#step7-1-减小过采样的数量，防止过拟合" class="headerlink" title="step7.1:减小过采样的数量，防止过拟合"></a>step7.1:减小过采样的数量，防止过拟合</h2><p>对/home/yf/data/clean.txt中每种类别进行统计各有多少个数：</p>
<table>
<thead>
<tr>
<th>每种类别包含图片张数</th>
<th>类别数</th>
<th>每种类别包含图片张数</th>
<th>类别数</th>
</tr>
</thead>
<tbody>
<tr>
<td>&lt;10</td>
<td>1213</td>
<td>10~20</td>
<td>11617</td>
</tr>
<tr>
<td>20~30</td>
<td>10868</td>
<td>30~40</td>
<td>9692</td>
</tr>
<tr>
<td>40~50</td>
<td>9020</td>
<td>50~60</td>
<td>8426</td>
</tr>
<tr>
<td>60~70</td>
<td>8443</td>
<td>70~80</td>
<td>8783</td>
</tr>
<tr>
<td>80~90</td>
<td>8762</td>
<td>90~100</td>
<td>7317</td>
</tr>
<tr>
<td>100~110</td>
<td>4277</td>
<td>110~120</td>
<td>1753</td>
</tr>
<tr>
<td>120~130</td>
<td>354</td>
<td></td>
</tr>
</tbody>
</table>
<p>类别总数共90525。由上图可知，类别严重不均衡，之前处理类别不均衡的方法主要是欠抽样和过抽样结合，对于多数类样本丢弃一部分样本，对于少数类样本复制生成，最后的训练数据分布如下：</p>
<table>
<thead>
<tr>
<th>每种类别包含图片张数</th>
<th>类别数</th>
<th>每种类别包含图片张数</th>
<th>类别数</th>
</tr>
</thead>
<tbody>
<tr>
<td>70~80</td>
<td>3673</td>
<td>80~90</td>
<td>19068</td>
</tr>
<tr>
<td>90~100</td>
<td>27069</td>
<td>100~110</td>
<td>40715</td>
</tr>
</tbody>
</table>
<p>由于少数类占了大多数，但是重复太多，可能导致过拟合问题，于是将每个类别的图片张数减去30，重新生成balance的训练数据，并训练模型。</p>
<p><u>迭代17万次后，msdata测试集上准确率达到92.28%，loss=0.27</u></p>
<p><u>lfw上精度为99.18%</u></p>
<p><u>mirror:99.27%</u></p>
<p><u>add:99.27%</u></p>
<h2 id="step7-2-EasyEmsemble法均衡类别"><a href="#step7-2-EasyEmsemble法均衡类别" class="headerlink" title="step7.2:EasyEmsemble法均衡类别"></a>step7.2:EasyEmsemble法均衡类别</h2><p>step7.1的方法属于欠抽样和过抽样结合：</p>
<ul>
<li>对于欠抽样算法，将多数类样本删除有可能会导致分类器<strong>丢失有关多数类的重要信息</strong>。</li>
<li>对于过抽样算法，虽然只是简单地将复制后的数据添加到原始数据集中，且某些样本的多个实例都是“<strong>并列的</strong>”，但这样也可能会导致分类器学习出现<strong>过拟合现象</strong>，对于同一个样本的多个复本产生多个规则条例，这就使得<strong>规则过于具体化</strong>；虽然在这种情况下，分类器的训练精度会很高，但在位置样本的分类性能就会非常不理想。</li>
</ul>
<p><strong>EasyEnsemble 核心思想是：</strong></p>
<ul>
<li><p>首先通过从多数类中<strong>独立随机</strong>抽取出若干子集</p>
</li>
<li><p>将每个子集与少数类数据<strong>联合</strong>起来<strong>训练</strong>生成多个基分类器</p>
</li>
<li><p>最终将这些基分类器<strong>组合形成</strong>一个集成学习系统</p>
<p>设立一个阈值50，对于类别样本数超过50的，将其分写到两个不同的文件；对于类别样本数不超过50的，利用过采样进行增添，所以最终得到两个有交集的训练集A,B，两个训练集的样本数都是</p>
<p>90525*50=4526250</p>
</li>
</ul>
<p><u>训练两个model，然后提取特征，对特征进行融合。</u></p>
<table>
<thead>
<tr>
<th>Model(acc/loss)</th>
<th>Pca Size</th>
<th>Threshold</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>model1(90.75%/0.41)</td>
<td>176</td>
<td>0.64</td>
<td>99.05%</td>
</tr>
<tr>
<td>model1(92.14%/0.26)</td>
<td>200</td>
<td>0.62</td>
<td>99.28%</td>
</tr>
<tr>
<td>model1(92.14%/0.26) Mirror</td>
<td>280</td>
<td>0.63</td>
<td>99.32%</td>
</tr>
<tr>
<td>model1(92.14%/0.26) Add Mirror</td>
<td>128</td>
<td>0.65</td>
<td>99.30%</td>
</tr>
<tr>
<td>model2(92.56%/0.41)</td>
<td>128</td>
<td>0.64</td>
<td>99.27%</td>
</tr>
<tr>
<td>model2(92.56%/0.41) Mirror</td>
<td>192</td>
<td>0.63</td>
<td>99.35%</td>
</tr>
<tr>
<td>model2(92.56%/0.41) Add Mirror</td>
<td>192</td>
<td>0.64</td>
<td>99.32%</td>
</tr>
<tr>
<td>model1 add model2</td>
<td>152</td>
<td>0.64</td>
<td>99.33%</td>
</tr>
<tr>
<td>model1 mirror add model2 mirror</td>
<td>136</td>
<td>0.65</td>
<td>99.37%</td>
</tr>
<tr>
<td>model1 add model2 mirror</td>
<td>152</td>
<td>0.64</td>
<td>99.37%</td>
</tr>
<tr>
<td>model1_add_mirror add model2_add_mirror</td>
<td>152</td>
<td>0.64</td>
<td>99.38%</td>
</tr>
<tr>
<td>model1_add_mirror concate model2_add_mirror</td>
<td>128</td>
<td>0.65</td>
<td>99.32%</td>
</tr>
<tr>
<td>model1 mirror min model2 mirror</td>
<td>168</td>
<td>0.64</td>
<td>99.37%</td>
</tr>
</tbody>
</table>
<h1 id="step8-UMDFaces"><a href="#step8-UMDFaces" class="headerlink" title="step8:UMDFaces"></a>step8:UMDFaces</h1><p>对UMDFaces数据集进行人脸对齐处理</p>
<p>batch1:175,534(3554类)</p>
<p>batch2:115,126(2590类)</p>
<p>batch3:77,228(2133类)</p>
<p>frames:3,735,475(3106类)</p>
<p>提取4个数据集的类别名称，经过处理分析后发现frames的类别属于batch1类别的子集，将3个batch与frames的数据集整合到一个数据集下，因为当静态图片和视频帧进行结合后训练的模型往往既能兼顾个体之间的差异（静态图片特征）也能学习到同一个个体的姿态变化（视频帧特征），要注意的一点就是对于frames和batch1中同一个类别的要放在一个目录下，并重新生成类别标签。</p>
<p>数据总量:4103363(8276个类别)</p>
<p>数据整理已经完成，接下来是在这个数据集上进行metric learning的训练。</p>
<p>train:3286012<br>val:817351</p>
<h1 id="step9-Megaface测试"><a href="#step9-Megaface测试" class="headerlink" title="step9:Megaface测试"></a>step9:Megaface测试</h1><table>
<thead>
<tr>
<th>Model</th>
<th>Dataset</th>
<th>Score(Megaface/LFW)</th>
</tr>
</thead>
<tbody>
<tr>
<td>center-face</td>
<td>FaceScrub Set1/LFW</td>
<td>67.32%/99.42%</td>
</tr>
<tr>
<td>balance-reduced</td>
<td>FaceScrub Set1/LFW</td>
<td>70.99%/99.18%</td>
</tr>
<tr>
<td>easyensemble</td>
<td>FaceScrub Set1</td>
<td>73.91%/99.33%</td>
</tr>
<tr>
<td>easyensemble  concat addmirror</td>
<td>FaceScrub Set1</td>
<td>74.21%/99.37%</td>
</tr>
<tr>
<td>balance-cent-soft</td>
<td>FaceScrub Set1/LFW</td>
<td>74.47%/99.33%</td>
</tr>
<tr>
<td>balance-cent-soft concat mirror</td>
<td>FaceScrub Set1</td>
<td>75.65%</td>
</tr>
</tbody>
</table>
<h2 id="step9-1-Megaface测试（续）"><a href="#step9-1-Megaface测试（续）" class="headerlink" title="step9.1:Megaface测试（续）"></a>step9.1:Megaface测试（续）</h2><table>
<thead>
<tr>
<th>Model</th>
<th>Dataset</th>
<th>Score(Megaface/LFW)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dropout_center Concat mirror</td>
<td>FaceScrub Set1/LFW</td>
<td>69.34%/99.47%</td>
</tr>
<tr>
<td>normface easyensemble model1 Concart mirror</td>
<td>FaceScrub Set1/LFW</td>
<td>70.32%</td>
</tr>
<tr>
<td>normface easyensemble 2models Concat mirror</td>
<td>FaceScrub Set1</td>
<td>70.49%</td>
</tr>
<tr>
<td>balance concat mirror</td>
<td>FaceScrub(matlab_mtcnn)</td>
<td>78.84%</td>
</tr>
<tr>
<td>easyensemble concat addmirror</td>
<td>FaceScrub(matlab_mtcnn)</td>
<td>75.92%</td>
</tr>
<tr>
<td>jitter_center_iter_190000 concat mirror</td>
<td>FaceScrub(matlab_mtcnn)</td>
<td>77.69%</td>
</tr>
<tr>
<td>jitter_softmax_iter_180000 concat mirror</td>
<td>FaceScrub(matlab_mtcnn)</td>
<td>83.13%</td>
</tr>
<tr>
<td>jitter_softmax_iter_180000 only mirror</td>
<td>FaceScrub(matlab_mtcnn)</td>
<td>82.08%</td>
</tr>
<tr>
<td>jitter_softmax_iter_180000 add mirror</td>
<td>FaceScrub(matlab_mtcnn)</td>
<td>82.87%</td>
</tr>
<tr>
<td>jitter_softmax_iter_184000 concat mirror</td>
<td>FaceScrub(matlab_mtcnn)</td>
<td>83.16%</td>
</tr>
<tr>
<td>jitter_softmax_iter_180000 concat mirror</td>
<td>FaceScrub(python_mtcnn)</td>
<td>78.33%</td>
</tr>
<tr>
<td>jitter_softmax_iter_184000 concat mirror(matlab align)</td>
<td>FaceScrub(matlab_mtcnn)</td>
<td>79.05%</td>
</tr>
<tr>
<td>normface_jitter_iter_124000 concat mirror(python align)</td>
<td>FaceScrub(python_mtcnn)</td>
<td>76.50%</td>
</tr>
<tr>
<td>normface_jitter_iter124000 cancat mirror(python align)</td>
<td>FaceScrub(matlab_mtcnn)</td>
<td>78.92%</td>
</tr>
</tbody>
</table>
<h1 id="step-10-MTCNN-matlab-人脸检测及对齐"><a href="#step-10-MTCNN-matlab-人脸检测及对齐" class="headerlink" title="step 10:MTCNN(matlab)人脸检测及对齐"></a>step 10:MTCNN(matlab)人脸检测及对齐</h1><h2 id="step-10-1：对齐Megaface和FaceScrub"><a href="#step-10-1：对齐Megaface和FaceScrub" class="headerlink" title="step 10.1：对齐Megaface和FaceScrub"></a>step 10.1：对齐Megaface和FaceScrub</h2><p>主要是Megaface数据集（1028062张）,FaceScrub数据集(91712张)，其中FaceScrub数据集中通过mtcnn（/home/yf/align/align_megaface.m）检测到的有89751张，剩余的1961张需要利用数据集中提供的3个关键点进行对齐，首先需要获取未检测到的图片的路径，然后利用python 脚本(/home/yf/megaface/devkit/templatelists/analysis/analysis_json.py)解析对应的存储该图片中人脸关键点的json文件，最后在利用matlab脚本(/home/yf/align/for_not_detect/align_megaface.m)进行批量对齐。Megaface数据集中未检测到的数据集同样处理。</p>
<p><strong>问题</strong>：</p>
<p>在进行了41万次对齐后，出现了imread的错误，然后将从目录读取路径改成了从存储图片路径的文件中(/home/yf/megaface/tests/MegaFace_align_list_image.txt)直接获取路径，并输出每次进行处理的文件名，重新进行对齐操作，然后重现了这个错误，最后比对MegaFace_align_list_image.txt的下一张图片，发现有张图片是输入为空的。</p>
<h2 id="step-10-2-对齐msceleb数据"><a href="#step-10-2-对齐msceleb数据" class="headerlink" title="step 10.2:对齐msceleb数据"></a>step 10.2:对齐msceleb数据</h2><p>重新对齐msceleb数据集用于训练。</p>
<h1 id="step-11-Normface训练"><a href="#step-11-Normface训练" class="headerlink" title="step 11:Normface训练"></a>step 11:Normface训练</h1><p>Normface(paper:<a href="https://arxiv.org/pdf/1704.06369.pdf" target="_blank" rel="external">NormFace: L2 Hypersphere Embedding for Face Verification</a>)</p>
<h2 id="step-11-1-训练EasyEnsemble模型"><a href="#step-11-1-训练EasyEnsemble模型" class="headerlink" title="step 11.1:训练EasyEnsemble模型"></a>step 11.1:训练EasyEnsemble模型</h2><p>model1在测试集上的准确率为92.88%，model2在测试集上的准确率为92.85%。</p>
<p>暂时只测了单个的model1 concate mirror在Megaface(还是原始python版mtcnn对齐的)上的准确率只有70.32%。下周继续测试两个模型的效果。</p>
<h2 id="step-11-2-训练Balance模型"><a href="#step-11-2-训练Balance模型" class="headerlink" title="step 11.2:训练Balance模型"></a>step 11.2:训练Balance模型</h2><p>刚生成完训练的数据集，下周开始训练。</p>
<h1 id="step-12-Image-Jitter"><a href="#step-12-Image-Jitter" class="headerlink" title="step 12:Image Jitter"></a>step 12:Image Jitter</h1><p>对图片增加随机扰动，包括缩放、角度变换、镜像操作，主要还是msceleb数据集(/home/yf/data/msclean)上进行，由于该数据集类别不均衡，所以对于样本数较少的类别可以采用这种办法增加样本容量。最终将每个类别的样本数控制在80~160之间。</p>
<p>10240892</p>
<p>train:9257534</p>
<p>val:983342</p>
<p>正在生成训练的数据集lmdb。</p>
<p>jitter_center_iter_190000 concat mirror  FaceScrub(matlab_mtcnn)  77.69%  </p>
<p>jitter_softmax_iter_180000 concat mirror  FaceScrub(matlab_mtcnn)  83.18%</p>
<p>jitter_softmax_iter_180000 only mirror  FaceScrub(matlab_mtcnn)  82.08%</p>
<p>jitter_softmax_iter_180000 add mirror  FaceScrub(matlab_mtcnn)  82.87%</p>
<p>jitter_softmax_iter_184000 concat mirror  FaceScrub(matlab_mtcnn)  83.16%</p>
<h1 id="step-13-Gender-test"><a href="#step-13-Gender-test" class="headerlink" title="step 13:Gender test"></a>step 13:Gender test</h1><h3 id="1-0"><a href="#1-0" class="headerlink" title="1.0"></a>1.0</h3><p>VGG16在lfw上准确率90.04%，在imdb(15590测试样本)上准确率90.92%</p>
<p>model training:female(69847),male(86061)</p>
<p>train:124728</p>
<p>val:15590</p>
<p>test:15590</p>
<h3 id="2-0"><a href="#2-0" class="headerlink" title="2.0"></a>2.0</h3><p>网络：AlexNet 在清理后的数据集上，迭代29500次后，训练准确率为98.16%，loss=0.55</p>
<p>在测试集上达到98.11%（15127/15418)</p>
<h1 id="Todo1"><a href="#Todo1" class="headerlink" title="Todo1"></a>Todo1</h1><ul>
<li>[x] Create umdfaces–&gt;lmdb</li>
<li>[x] EasyEnsemble train and test</li>
<li>[x] Use matcaffe for metric learning</li>
<li>[ ] Megaface test<pre><code>- - [x] center face
  - [x] balance-cent-soft
  - [x] reduced
  - [x] mirror or concatenate
  - [x] EasyEnsemble
</code></pre></li>
</ul>
<ul>
<li><p>[x] Paper reading:One-shot face recognition by promoting underrepresented classes</p>
<pre><code>​
</code></pre></li>
</ul>
<h1 id="Todo2"><a href="#Todo2" class="headerlink" title="Todo2"></a>Todo2</h1><ul>
<li><p>[ ] jitter model training(softmax first)</p>
</li>
<li><p>[ ] balance model retrain on normface(include center loss)</p>
</li>
<li><p>[ ] aligned by matlab_mtcnn megaface(balance model 75.65% version)</p>
</li>
<li><p>[ ] gender classfication model training</p>
<pre><code>- check the dataset (detect and crop by matlab_mtcnn)

- generate lmdb

- choose a model(ResNet?)

  ​
</code></pre></li>
</ul>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><p><a href="https://github.com/happynear/FaceVerification" target="_blank" rel="external">happynear-face-verification</a></p>
<p><a href="https://github.com/davisking/dlib/blob/cbd187fb6109d21406f6a76bb0e9aa0689b1e54a/examples/dnn_face_recognition_ex.cpp" target="_blank" rel="external">dlib-jitter</a></p>
<p><a href="http://blog.dlib.net" target="_blank" rel="external">dlib-face-verification-blog</a></p>
]]></content>
    
    <summary type="html">
    
      人脸识别实验记录
    
    </summary>
    
      <category term="实验记录" scheme="calyp.github.io/categories/%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95/"/>
    
    
  </entry>
  
  <entry>
    <title>杂知识点</title>
    <link href="calyp.github.io/2017/06/01/%E6%9D%82%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>calyp.github.io/2017/06/01/杂知识点/</id>
    <published>2017-06-01T01:42:05.000Z</published>
    <updated>2019-03-01T03:19:23.990Z</updated>
    
    <content type="html"><![CDATA[<h1 id="分类与回归"><a href="#分类与回归" class="headerlink" title="分类与回归"></a>分类与回归</h1><p><strong>本节部分转载于穆文发表于知乎的<a href="https://www.zhihu.com/question/21329754/answer/151216012" target="_blank" rel="external">分类与回归区别是什么</a>下面的回答，获得原作者授权</strong></p>
<p><em>分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。</em></p>
<ol>
<li><p>Logistic Regression&amp;Linear Regression:</p>
<ul>
<li>Linear Regression:输出一个标量<strong>wx+b</strong>，这个值是连续值，用以回归问题</li>
<li>Logistic Regression:将上面的<strong>wx+b</strong>通过<strong>sigmoid</strong>函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题</li>
<li>对于N分类问题，可以先计算N组w值不同的<strong>wx+b</strong> ，然后归一化，比如<strong>softmax</strong>函数变成N个类上的概率，用以多分类</li>
</ul>
</li>
<li><p>SVR &amp;SVM</p>
<ul>
<li>SVR:输出<strong>wx+b</strong>，即某个样本点到分类面的距离，是连续值，属于回归问题</li>
<li>SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类</li>
</ul>
</li>
<li><p>Naive Bayes用来分类和回归</p>
</li>
<li><p>前馈神经网络（CNN系列）用于分类和回归</p>
<ul>
<li>回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出看做向量v，现全部连接到一个神经元上，这个神经元的输出<strong>wv+b</strong>，是一个连续值，处理回归问题，和Linear Regression的思想一样</li>
<li>分类：将m个神经元最后连接到N个神经元，有N组不同的<strong>wv+b</strong>，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题</li>
</ul>
</li>
<li><p>循环神经网络（RNN系列）用于分类和回归</p>
<ul>
<li>回归和分类与CNN类似，输出层的值<strong>y=wx+b</strong>，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列</li>
</ul>
</li>
<li><p>Logistic回归&amp;SVM<br> 两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。<a href="[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)">^1</a><br> 线性模型的表达式为<br>$$<br>h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n<br>$$</p>
</li>
</ol>
<p>​    将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。</p>
<p>​    用n表示Feature数量,m表示训练集个数。下面分情况讨论<a href="[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)">^2</a>：</p>
<ul>
<li>n很大，m很小<br>n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。</li>
<li>n很小，m中等<br>n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择<code>非线性kernel的SVM</code>，比如<code>高斯核kernel的SVM</code>。</li>
<li>n很小，m很大<br>n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，<code>带核函数的SVM</code>计算也非常慢。所以此时应该选<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。</li>
</ul>
<h1 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h1><h2 id="迁移学习-3："><a href="#迁移学习-3：" class="headerlink" title="迁移学习^3："></a>迁移学习<a href="[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]">^3</a>：</h2><p>有监督预训练(<em>supervised pre-training</em>)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。</p>
<ul>
<li><strong>NMS(非极大值抑制)：</strong><br>在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列<a href="http://lib.csdn.net/base/datastructure" target="_blank" rel="external">算法</a>中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。</li>
</ul>
<p>定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。<br>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;<br>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。<br>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。<br>就这样一直重复，找到所有被保留下来的矩形框。</p>
<ul>
<li><strong>IoU(交并比)：</strong><br>物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们<a href="http://lib.csdn.net/base/datastructure" target="_blank" rel="external">算法</a>不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式<br>$$<br>IoU=\frac{A\cap B}{A\cup B}<br>$$</li>
</ul>
<h2 id="准确率-amp-精确率-amp-召回率"><a href="#准确率-amp-精确率-amp-召回率" class="headerlink" title="准确率&amp;精确率&amp;召回率:"></a>准确率&amp;精确率&amp;召回率:</h2><p>​    <em>准确率是正确预测的样本占总的预测样本比例</em><br>​    <em>精确率是预测为正的样本中有多少是真的正类</em><br>​    <em>召回率是样本中有多少正例被正确的预测</em><br>​    <em>F值=准确率*召回率*2/(准确率+召回率)，是准确率和召回率的调和平均值</em><br>​<em>TP</em>：正类被预测为正类<br>​<em>FN</em>：正类被预测为负类<br>​<em>FP</em>：负类被预测为正类<br>​<em>TN</em>：负类被预测为负类</p>
<p>$$<br>准确率=\frac{TP+TN}{TP+TF+FN+FP}<br>$$</p>
<p>$$<br>精确率=\frac{TP}{TP+FP}<br>$$</p>
<p>$$<br>召回率=\frac{TP}{TP+FN}<br>$$</p>
<h2 id="卷积计算后的图片尺寸："><a href="#卷积计算后的图片尺寸：" class="headerlink" title="卷积计算后的图片尺寸："></a>卷积计算后的图片尺寸：</h2><p>$$<br>outputsize=\frac{imagesize+2*padding-kernelsize}{stride}+1<br>$$</p>
<h1 id="RankBoost"><a href="#RankBoost" class="headerlink" title="RankBoost:"></a>RankBoost:</h1><p>​    RankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类    器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1&gt;r2&gt;r3&gt;r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题</p>
<h1 id="真阳率，假阳率，AUC，ROC"><a href="#真阳率，假阳率，AUC，ROC" class="headerlink" title="真阳率，假阳率，AUC，ROC"></a>真阳率，假阳率，AUC，ROC</h1><p><img src="http://img.blog.csdn.net/20150919111349931" alt="pic1"></p>
<p>$真阳率=\frac{a}{a+c}$:含义是检测出来的真阳性样本数除以所有真实阳性样本数。</p>
<p>$假阳率=\frac{b}{b+d}$:含义是检测出来的假阳性样本数除以所有真实阴性样本数</p>
<p>ROC曲线就是把假阳率当x轴，真阳率当y轴画一个二维平面直角坐标系。然后不断调整检测方法（或机器学习中的分类器）的阈值，即最终得分高于某个值就是阳性，反之就是阴性，得到不同的真阳率和假阳率数值，然后描点。就可以得到一条ROC曲线。<br>需要注意的是，ROC曲线必定起于（0，0），止于（1，1）。因为，当全都判断为阴性(-)时，就是（0，0）；全部判断为阳性(+)时就是（1，1）。这两点间斜率为1的线段表示随机分类器（对真实的正负样本没有区分能力）。所以一般分类器需要在这条线上方</p>
<p><img src="http://img.blog.csdn.net/20150919114145488" alt="pic2"></p>
<p>AUC就是ROC曲线下方的面积，越接近1表示分类器越好。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://www.zhihu.com/question/21704547/answer/20293255" target="_blank" rel="external">SVM和logistic回归分别在什么情况下使用</a></p>
<p><a href="http://blog.csdn.net/ybdesire/article/details/54143481" target="_blank" rel="external">SVM和Logistic的区别</a></p>
<p>[<a href="http://blog.csdn.net/zhang_shuai12/article/details/52716952" target="_blank" rel="external">物体检测中常用的几个概念迁移学习、IOU、NMS理解</a>]</p>
<p><a href="http://www.cnblogs.com/maybe2030/p/5375175.html" target="_blank" rel="external">机器学习算法常用指标总结</a></p>
]]></content>
    
    <summary type="html">
    
      深度学习的一些基础知识点
    
    </summary>
    
      <category term="深度学习" scheme="calyp.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="calyp.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="calyp.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
